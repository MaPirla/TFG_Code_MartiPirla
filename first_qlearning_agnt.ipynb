{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from qlearningAgent import QlearningAgent\n",
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_debug = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ColumnGame():\n",
    "    def __init__(self, agent: QlearningAgent, depth: int = 2):\n",
    "        self.agent = agent\n",
    "        self.gain = 50\n",
    "        self.depth = depth\n",
    "        self.acumulated_reward = 0\n",
    "        self.action_set = [\n",
    "            #'R', # Choose the column on the right\n",
    "            #'L', # Choose the column on the left\n",
    "            'B', # Choose the bigger column\n",
    "            'S', # Choose the smaller column\n",
    "            #'1', # Choose the first apearing column\n",
    "            #'2'  # Choose the second column to apear\n",
    "            ]\n",
    "\n",
    "    def start_game(self):\n",
    "        col_sizes = (\n",
    "            self.gain + self.gain*0.4,\n",
    "            self.gain + self.gain*-0.4\n",
    "        )\n",
    "        state = (\n",
    "            0, # Current depth\n",
    "            self.gain,\n",
    "            np.random.choice([\"R\", \"L\"]), # Bigger column\n",
    "            np.random.choice([\"R\", \"L\"]) # First column to appear\n",
    "        )\n",
    "\n",
    "        df= pd.DataFrame(columns=[\"horizon\", \"current_depth\", \"chosen_big\", \"chosen_right\", \"chosen_first\", \"performance\"])\n",
    "        for i in range(self.depth):\n",
    "            if print_debug: print(\"\\n\\nState: depth:\", state[0], \", Bigger Column:\", state[1], \", First column to appear:\", state[2])\n",
    "            action = self.agent.take_action_learn(state, self.action_set)\n",
    "            if print_debug: print(\"the agent chose the column that is \", action)\n",
    "            col = 0\n",
    "            match action:\n",
    "                case \"B\":\n",
    "                    col = 0\n",
    "                case \"S\":\n",
    "                    col = 1\n",
    "                case \"R\":\n",
    "                    if state[1] != \"R\": col = 1\n",
    "                case \"L\":\n",
    "                    if state[1] != \"L\": col = 1\n",
    "                case \"1\":\n",
    "                    if state[1] != state[2]: col = 1\n",
    "                case \"2\":\n",
    "                    pos = \"R\"\n",
    "                    if state[2] == \"R\": pos = \"L\"\n",
    "                    if state[1] != pos: col = 1\n",
    "            \n",
    "            if col == 0:\n",
    "                self.gain /= 5\n",
    "                if print_debug: print(\"the gain is reduced to\", self.gain)\n",
    "            else:\n",
    "                self.gain *= 5\n",
    "                if print_debug: print(\"The gain is incremented to\", self.gain)\n",
    "            reward = col_sizes[col]\n",
    "            if print_debug and col == 0: print(\"The agent chose the bigger column, , giving a reward of\", col_sizes[0])\n",
    "            if print_debug and col == 1: print(\"The agent chose the smaller column, giving a reward of\", col_sizes[0])\n",
    "            col_sizes = (\n",
    "                self.gain + self.gain*0.1,\n",
    "                self.gain + self.gain*-0.1\n",
    "            )\n",
    "            state = (\n",
    "                i+1, # Current depth\n",
    "                self.gain,\n",
    "                np.random.choice([\"R\", \"L\"]), # Bigger column\n",
    "                np.random.choice([\"R\", \"L\"]) # First column to appear\n",
    "            )\n",
    "            self.acumulated_reward += reward\n",
    "            if print_debug: print(\"The current acumulated reward is\", self.acumulated_reward)\n",
    "            self.agent.update_qtable(reward, state, self.action_set)\n",
    "            performance = 0\n",
    "            if self.depth == i+1:\n",
    "                if col == 0:\n",
    "                    performance = 1\n",
    "            else:\n",
    "                if col == 1:\n",
    "                    performance = 1\n",
    "            new_record = pd.DataFrame([{\"horizon\": self.depth, \"current_depth\": i+1, \"chosen_big\": col == 0, \"chosen_right\": (col == 0) == (state[1] == \"R\"), \"chosen_first\": (col == 0) == (state[2] == \"R\"), \"performance\": performance}])\n",
    "            df = pd.concat([df, new_record], ignore_index=True)\n",
    "        if print_debug: print(\"END OF THE EPISODE _______________________________________________\\n\\n\")\n",
    "        self.agent.clear_next_episode()\n",
    "        return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "State: depth: 0 , Bigger Column: 50 , First column to appear: L\n",
      "current_state setted to None\n",
      "exploratory\n",
      "the agent chose the column that is  S\n",
      "The gain is incremented to 250\n",
      "The agent chose the smaller column, giving a reward of 70.0\n",
      "The current acumulated reward is 30.0\n",
      "Inmediate reward: 30.0\n",
      "update 0.0 =  0.0 + 0.8 *( 30.0 + 0.9 * 0.0 - 0.0 )\n",
      "The new value of ((0, 50, 'L', 'R'), 'S') is  24.0\n",
      "\n",
      "\n",
      "State: depth: 1 , Bigger Column: 250 , First column to appear: R\n",
      "current_state setted to ((0, 50, 'L', 'R'), 'S')\n",
      "exploratory\n",
      "the agent chose the column that is  B\n",
      "the gain is reduced to 50.0\n",
      "The agent chose the bigger column, , giving a reward of 275.0\n",
      "The current acumulated reward is 305.0\n",
      "Inmediate reward: 275.0\n",
      "update 0.0 =  0.0 + 0.8 *( 275.0 + 0.9 * 0.0 - 0.0 )\n",
      "The new value of ((1, 250, 'R', 'L'), 'B') is  220.0\n",
      "END OF THE EPISODE _______________________________________________\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "State: depth: 0 , Bigger Column: 50 , First column to appear: R\n",
      "current_state setted to None\n",
      "exploratory\n",
      "the agent chose the column that is  S\n",
      "The gain is incremented to 250\n",
      "The agent chose the smaller column, giving a reward of 70.0\n",
      "The current acumulated reward is 30.0\n",
      "Inmediate reward: 30.0\n",
      "update 0.0 =  0.0 + 0.8 *( 30.0 + 0.9 * 0.0 - 0.0 )\n",
      "The new value of ((0, 50, 'R', 'R'), 'S') is  24.0\n",
      "\n",
      "\n",
      "State: depth: 1 , Bigger Column: 250 , First column to appear: L\n",
      "current_state setted to ((0, 50, 'R', 'R'), 'S')\n",
      "exploratory\n",
      "the agent chose the column that is  B\n",
      "the gain is reduced to 50.0\n",
      "The agent chose the bigger column, , giving a reward of 275.0\n",
      "The current acumulated reward is 305.0\n",
      "Inmediate reward: 275.0\n",
      "update 0.0 =  0.0 + 0.8 *( 275.0 + 0.9 * 0.0 - 0.0 )\n",
      "The new value of ((1, 250, 'L', 'L'), 'B') is  220.0\n",
      "END OF THE EPISODE _______________________________________________\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "State: depth: 0 , Bigger Column: 50 , First column to appear: L\n",
      "current_state setted to None\n",
      "exploratory\n",
      "the agent chose the column that is  B\n",
      "the gain is reduced to 10.0\n",
      "The agent chose the bigger column, , giving a reward of 70.0\n",
      "The current acumulated reward is 70.0\n",
      "Inmediate reward: 70.0\n",
      "update 0.0 =  0.0 + 0.8 *( 70.0 + 0.9 * 0.0 - 0.0 )\n",
      "The new value of ((0, 50, 'L', 'R'), 'B') is  56.0\n",
      "\n",
      "\n",
      "State: depth: 1 , Bigger Column: 10.0 , First column to appear: L\n",
      "current_state setted to ((0, 50, 'L', 'R'), 'B')\n",
      "Player looks for best value\n",
      "the agent chose the column that is  B\n",
      "the gain is reduced to 2.0\n",
      "The agent chose the bigger column, , giving a reward of 11.0\n",
      "The current acumulated reward is 81.0\n",
      "Inmediate reward: 11.0\n",
      "update 0.0 =  0.0 + 0.8 *( 11.0 + 0.9 * 0.0 - 0.0 )\n",
      "The new value of ((1, 10.0, 'L', 'R'), 'B') is  8.8\n",
      "END OF THE EPISODE _______________________________________________\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "State: depth: 0 , Bigger Column: 50 , First column to appear: L\n",
      "current_state setted to None\n",
      "exploratory\n",
      "the agent chose the column that is  B\n",
      "the gain is reduced to 10.0\n",
      "The agent chose the bigger column, , giving a reward of 70.0\n",
      "The current acumulated reward is 70.0\n",
      "Inmediate reward: 70.0\n",
      "update 56.0 =  56.0 + 0.8 *( 70.0 + 0.9 * 8.8 - 56.0 )\n",
      "The new value of ((0, 50, 'L', 'R'), 'B') is  73.536\n",
      "\n",
      "\n",
      "State: depth: 1 , Bigger Column: 10.0 , First column to appear: L\n",
      "current_state setted to ((0, 50, 'L', 'R'), 'B')\n",
      "Player looks for best value\n",
      "the agent chose the column that is  B\n",
      "the gain is reduced to 2.0\n",
      "The agent chose the bigger column, , giving a reward of 11.0\n",
      "The current acumulated reward is 81.0\n",
      "Inmediate reward: 11.0\n",
      "update 8.8 =  8.8 + 0.8 *( 11.0 + 0.9 * 0.0 - 8.8 )\n",
      "The new value of ((1, 10.0, 'L', 'R'), 'B') is  10.56\n",
      "END OF THE EPISODE _______________________________________________\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "State: depth: 0 , Bigger Column: 50 , First column to appear: L\n",
      "current_state setted to None\n",
      "Player looks for best value\n",
      "the agent chose the column that is  B\n",
      "the gain is reduced to 10.0\n",
      "The agent chose the bigger column, , giving a reward of 70.0\n",
      "The current acumulated reward is 70.0\n",
      "Inmediate reward: 70.0\n",
      "update 73.536 =  73.536 + 0.8 *( 70.0 + 0.9 * 0.0 - 73.536 )\n",
      "The new value of ((0, 50, 'L', 'R'), 'B') is  70.7072\n",
      "\n",
      "\n",
      "State: depth: 1 , Bigger Column: 10.0 , First column to appear: R\n",
      "current_state setted to ((0, 50, 'L', 'R'), 'B')\n",
      "Player looks for best value\n",
      "the agent chose the column that is  B\n",
      "the gain is reduced to 2.0\n",
      "The agent chose the bigger column, , giving a reward of 11.0\n",
      "The current acumulated reward is 81.0\n",
      "Inmediate reward: 11.0\n",
      "update 0.0 =  0.0 + 0.8 *( 11.0 + 0.9 * 0.0 - 0.0 )\n",
      "The new value of ((1, 10.0, 'R', 'R'), 'B') is  8.8\n",
      "END OF THE EPISODE _______________________________________________\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "State: depth: 0 , Bigger Column: 50 , First column to appear: L\n",
      "current_state setted to None\n",
      "exploratory\n",
      "the agent chose the column that is  B\n",
      "the gain is reduced to 10.0\n",
      "The agent chose the bigger column, , giving a reward of 70.0\n",
      "The current acumulated reward is 70.0\n",
      "Inmediate reward: 70.0\n",
      "update 70.7072 =  70.7072 + 0.8 *( 70.0 + 0.9 * 0.0 - 70.7072 )\n",
      "The new value of ((0, 50, 'L', 'R'), 'B') is  70.14144\n",
      "\n",
      "\n",
      "State: depth: 1 , Bigger Column: 10.0 , First column to appear: L\n",
      "current_state setted to ((0, 50, 'L', 'R'), 'B')\n",
      "exploratory\n",
      "the agent chose the column that is  S\n",
      "The gain is incremented to 50.0\n",
      "The agent chose the smaller column, giving a reward of 11.0\n",
      "The current acumulated reward is 79.0\n",
      "Inmediate reward: 9.0\n",
      "update 0.0 =  0.0 + 0.8 *( 9.0 + 0.9 * 0.0 - 0.0 )\n",
      "The new value of ((1, 10.0, 'L', 'L'), 'S') is  7.2\n",
      "END OF THE EPISODE _______________________________________________\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "State: depth: 0 , Bigger Column: 50 , First column to appear: R\n",
      "current_state setted to None\n",
      "exploratory\n",
      "the agent chose the column that is  S\n",
      "The gain is incremented to 250\n",
      "The agent chose the smaller column, giving a reward of 70.0\n",
      "The current acumulated reward is 30.0\n",
      "Inmediate reward: 30.0\n",
      "update 24.0 =  24.0 + 0.8 *( 30.0 + 0.9 * 220.0 - 24.0 )\n",
      "The new value of ((0, 50, 'R', 'R'), 'S') is  187.20000000000002\n",
      "\n",
      "\n",
      "State: depth: 1 , Bigger Column: 250 , First column to appear: R\n",
      "current_state setted to ((0, 50, 'R', 'R'), 'S')\n",
      "Player looks for best value\n",
      "the agent chose the column that is  B\n",
      "the gain is reduced to 50.0\n",
      "The agent chose the bigger column, , giving a reward of 275.0\n",
      "The current acumulated reward is 305.0\n",
      "Inmediate reward: 275.0\n",
      "update 220.0 =  220.0 + 0.8 *( 275.0 + 0.9 * 0.0 - 220.0 )\n",
      "The new value of ((1, 250, 'R', 'L'), 'B') is  264.0\n",
      "END OF THE EPISODE _______________________________________________\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "State: depth: 0 , Bigger Column: 50 , First column to appear: R\n",
      "current_state setted to None\n",
      "Player looks for best value\n",
      "the agent chose the column that is  S\n",
      "The gain is incremented to 250\n",
      "The agent chose the smaller column, giving a reward of 70.0\n",
      "The current acumulated reward is 30.0\n",
      "Inmediate reward: 30.0\n",
      "update 187.20000000000002 =  187.20000000000002 + 0.8 *( 30.0 + 0.9 * 220.0 - 187.20000000000002 )\n",
      "The new value of ((0, 50, 'R', 'R'), 'S') is  219.84\n",
      "\n",
      "\n",
      "State: depth: 1 , Bigger Column: 250 , First column to appear: L\n",
      "current_state setted to ((0, 50, 'R', 'R'), 'S')\n",
      "exploratory\n",
      "the agent chose the column that is  B\n",
      "the gain is reduced to 50.0\n",
      "The agent chose the bigger column, , giving a reward of 275.0\n",
      "The current acumulated reward is 305.0\n",
      "Inmediate reward: 275.0\n",
      "update 220.0 =  220.0 + 0.8 *( 275.0 + 0.9 * 0.0 - 220.0 )\n",
      "The new value of ((1, 250, 'L', 'L'), 'B') is  264.0\n",
      "END OF THE EPISODE _______________________________________________\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "State: depth: 0 , Bigger Column: 50 , First column to appear: L\n",
      "current_state setted to None\n",
      "exploratory\n",
      "the agent chose the column that is  B\n",
      "the gain is reduced to 10.0\n",
      "The agent chose the bigger column, , giving a reward of 70.0\n",
      "The current acumulated reward is 70.0\n",
      "Inmediate reward: 70.0\n",
      "update 70.14144 =  70.14144 + 0.8 *( 70.0 + 0.9 * 0.0 - 70.14144 )\n",
      "The new value of ((0, 50, 'L', 'R'), 'B') is  70.028288\n",
      "\n",
      "\n",
      "State: depth: 1 , Bigger Column: 10.0 , First column to appear: R\n",
      "current_state setted to ((0, 50, 'L', 'R'), 'B')\n",
      "exploratory\n",
      "the agent chose the column that is  S\n",
      "The gain is incremented to 50.0\n",
      "The agent chose the smaller column, giving a reward of 11.0\n",
      "The current acumulated reward is 79.0\n",
      "Inmediate reward: 9.0\n",
      "update 0.0 =  0.0 + 0.8 *( 9.0 + 0.9 * 0.0 - 0.0 )\n",
      "The new value of ((1, 10.0, 'R', 'L'), 'S') is  7.2\n",
      "END OF THE EPISODE _______________________________________________\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "State: depth: 0 , Bigger Column: 50 , First column to appear: L\n",
      "current_state setted to None\n",
      "Player looks for best value\n",
      "the agent chose the column that is  B\n",
      "the gain is reduced to 10.0\n",
      "The agent chose the bigger column, , giving a reward of 70.0\n",
      "The current acumulated reward is 70.0\n",
      "Inmediate reward: 70.0\n",
      "update 70.028288 =  70.028288 + 0.8 *( 70.0 + 0.9 * 7.2 - 70.028288 )\n",
      "The new value of ((0, 50, 'L', 'R'), 'B') is  75.1896576\n",
      "\n",
      "\n",
      "State: depth: 1 , Bigger Column: 10.0 , First column to appear: R\n",
      "current_state setted to ((0, 50, 'L', 'R'), 'B')\n",
      "Player looks for best value\n",
      "the agent chose the column that is  S\n",
      "The gain is incremented to 50.0\n",
      "The agent chose the smaller column, giving a reward of 11.0\n",
      "The current acumulated reward is 79.0\n",
      "Inmediate reward: 9.0\n",
      "update 7.2 =  7.2 + 0.8 *( 9.0 + 0.9 * 0.0 - 7.2 )\n",
      "The new value of ((1, 10.0, 'R', 'L'), 'S') is  8.64\n",
      "END OF THE EPISODE _______________________________________________\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "State: depth: 0 , Bigger Column: 50 , First column to appear: R\n",
      "current_state setted to None\n",
      "exploratory\n",
      "the agent chose the column that is  B\n",
      "the gain is reduced to 10.0\n",
      "The agent chose the bigger column, , giving a reward of 70.0\n",
      "The current acumulated reward is 70.0\n",
      "Inmediate reward: 70.0\n",
      "update 0.0 =  0.0 + 0.8 *( 70.0 + 0.9 * 10.56 - 0.0 )\n",
      "The new value of ((0, 50, 'R', 'R'), 'B') is  63.60320000000001\n",
      "\n",
      "\n",
      "State: depth: 1 , Bigger Column: 10.0 , First column to appear: L\n",
      "current_state setted to ((0, 50, 'R', 'R'), 'B')\n",
      "exploratory\n",
      "the agent chose the column that is  B\n",
      "the gain is reduced to 2.0\n",
      "The agent chose the bigger column, , giving a reward of 11.0\n",
      "The current acumulated reward is 81.0\n",
      "Inmediate reward: 11.0\n",
      "update 10.56 =  10.56 + 0.8 *( 11.0 + 0.9 * 0.0 - 10.56 )\n",
      "The new value of ((1, 10.0, 'L', 'R'), 'B') is  10.912\n",
      "END OF THE EPISODE _______________________________________________\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "State: depth: 0 , Bigger Column: 50 , First column to appear: R\n",
      "current_state setted to None\n",
      "Player looks for best value\n",
      "the agent chose the column that is  S\n",
      "The gain is incremented to 250\n",
      "The agent chose the smaller column, giving a reward of 70.0\n",
      "The current acumulated reward is 30.0\n",
      "Inmediate reward: 30.0\n",
      "update 219.84 =  219.84 + 0.8 *( 30.0 + 0.9 * 0.0 - 219.84 )\n",
      "The new value of ((0, 50, 'R', 'R'), 'S') is  67.96799999999999\n",
      "\n",
      "\n",
      "State: depth: 1 , Bigger Column: 250 , First column to appear: L\n",
      "current_state setted to ((0, 50, 'R', 'R'), 'S')\n",
      "exploratory\n",
      "the agent chose the column that is  S\n",
      "The gain is incremented to 1250\n",
      "The agent chose the smaller column, giving a reward of 275.0\n",
      "The current acumulated reward is 255.0\n",
      "Inmediate reward: 225.0\n",
      "update 0.0 =  0.0 + 0.8 *( 225.0 + 0.9 * 0.0 - 0.0 )\n",
      "The new value of ((1, 250, 'L', 'R'), 'S') is  180.0\n",
      "END OF THE EPISODE _______________________________________________\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "State: depth: 0 , Bigger Column: 50 , First column to appear: L\n",
      "current_state setted to None\n",
      "Player looks for best value\n",
      "the agent chose the column that is  B\n",
      "the gain is reduced to 10.0\n",
      "The agent chose the bigger column, , giving a reward of 70.0\n",
      "The current acumulated reward is 70.0\n",
      "Inmediate reward: 70.0\n",
      "update 0.0 =  0.0 + 0.8 *( 70.0 + 0.9 * 8.64 - 0.0 )\n",
      "The new value of ((0, 50, 'L', 'L'), 'B') is  62.2208\n",
      "\n",
      "\n",
      "State: depth: 1 , Bigger Column: 10.0 , First column to appear: R\n",
      "current_state setted to ((0, 50, 'L', 'L'), 'B')\n",
      "Player looks for best value\n",
      "the agent chose the column that is  S\n",
      "The gain is incremented to 50.0\n",
      "The agent chose the smaller column, giving a reward of 11.0\n",
      "The current acumulated reward is 79.0\n",
      "Inmediate reward: 9.0\n",
      "update 8.64 =  8.64 + 0.8 *( 9.0 + 0.9 * 0.0 - 8.64 )\n",
      "The new value of ((1, 10.0, 'R', 'L'), 'S') is  8.928\n",
      "END OF THE EPISODE _______________________________________________\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "State: depth: 0 , Bigger Column: 50 , First column to appear: L\n",
      "current_state setted to None\n",
      "Player looks for best value\n",
      "the agent chose the column that is  B\n",
      "the gain is reduced to 10.0\n",
      "The agent chose the bigger column, , giving a reward of 70.0\n",
      "The current acumulated reward is 70.0\n",
      "Inmediate reward: 70.0\n",
      "update 62.2208 =  62.2208 + 0.8 *( 70.0 + 0.9 * 8.928 - 62.2208 )\n",
      "The new value of ((0, 50, 'L', 'L'), 'B') is  74.87232\n",
      "\n",
      "\n",
      "State: depth: 1 , Bigger Column: 10.0 , First column to appear: R\n",
      "current_state setted to ((0, 50, 'L', 'L'), 'B')\n",
      "exploratory\n",
      "the agent chose the column that is  S\n",
      "The gain is incremented to 50.0\n",
      "The agent chose the smaller column, giving a reward of 11.0\n",
      "The current acumulated reward is 79.0\n",
      "Inmediate reward: 9.0\n",
      "update 8.928 =  8.928 + 0.8 *( 9.0 + 0.9 * 0.0 - 8.928 )\n",
      "The new value of ((1, 10.0, 'R', 'L'), 'S') is  8.9856\n",
      "END OF THE EPISODE _______________________________________________\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "State: depth: 0 , Bigger Column: 50 , First column to appear: R\n",
      "current_state setted to None\n",
      "exploratory\n",
      "the agent chose the column that is  S\n",
      "The gain is incremented to 250\n",
      "The agent chose the smaller column, giving a reward of 70.0\n",
      "The current acumulated reward is 30.0\n",
      "Inmediate reward: 30.0\n",
      "update 0.0 =  0.0 + 0.8 *( 30.0 + 0.9 * 264.0 - 0.0 )\n",
      "The new value of ((0, 50, 'R', 'L'), 'S') is  214.08000000000004\n",
      "\n",
      "\n",
      "State: depth: 1 , Bigger Column: 250 , First column to appear: R\n",
      "current_state setted to ((0, 50, 'R', 'L'), 'S')\n",
      "Player looks for best value\n",
      "the agent chose the column that is  B\n",
      "the gain is reduced to 50.0\n",
      "The agent chose the bigger column, , giving a reward of 275.0\n",
      "The current acumulated reward is 305.0\n",
      "Inmediate reward: 275.0\n",
      "update 264.0 =  264.0 + 0.8 *( 275.0 + 0.9 * 0.0 - 264.0 )\n",
      "The new value of ((1, 250, 'R', 'L'), 'B') is  272.8\n",
      "END OF THE EPISODE _______________________________________________\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "State: depth: 0 , Bigger Column: 50 , First column to appear: L\n",
      "current_state setted to None\n",
      "exploratory\n",
      "the agent chose the column that is  B\n",
      "the gain is reduced to 10.0\n",
      "The agent chose the bigger column, , giving a reward of 70.0\n",
      "The current acumulated reward is 70.0\n",
      "Inmediate reward: 70.0\n",
      "update 75.1896576 =  75.1896576 + 0.8 *( 70.0 + 0.9 * 8.8 - 75.1896576 )\n",
      "The new value of ((0, 50, 'L', 'R'), 'B') is  77.37393152\n",
      "\n",
      "\n",
      "State: depth: 1 , Bigger Column: 10.0 , First column to appear: R\n",
      "current_state setted to ((0, 50, 'L', 'R'), 'B')\n",
      "Player looks for best value\n",
      "the agent chose the column that is  B\n",
      "the gain is reduced to 2.0\n",
      "The agent chose the bigger column, , giving a reward of 11.0\n",
      "The current acumulated reward is 81.0\n",
      "Inmediate reward: 11.0\n",
      "update 8.8 =  8.8 + 0.8 *( 11.0 + 0.9 * 0.0 - 8.8 )\n",
      "The new value of ((1, 10.0, 'R', 'R'), 'B') is  10.56\n",
      "END OF THE EPISODE _______________________________________________\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "State: depth: 0 , Bigger Column: 50 , First column to appear: R\n",
      "current_state setted to None\n",
      "exploratory\n",
      "the agent chose the column that is  B\n",
      "the gain is reduced to 10.0\n",
      "The agent chose the bigger column, , giving a reward of 70.0\n",
      "The current acumulated reward is 70.0\n",
      "Inmediate reward: 70.0\n",
      "update 0.0 =  0.0 + 0.8 *( 70.0 + 0.9 * 7.2 - 0.0 )\n",
      "The new value of ((0, 50, 'R', 'L'), 'B') is  61.184000000000005\n",
      "\n",
      "\n",
      "State: depth: 1 , Bigger Column: 10.0 , First column to appear: L\n",
      "current_state setted to ((0, 50, 'R', 'L'), 'B')\n",
      "exploratory\n",
      "the agent chose the column that is  S\n",
      "The gain is incremented to 50.0\n",
      "The agent chose the smaller column, giving a reward of 11.0\n",
      "The current acumulated reward is 79.0\n",
      "Inmediate reward: 9.0\n",
      "update 7.2 =  7.2 + 0.8 *( 9.0 + 0.9 * 0.0 - 7.2 )\n",
      "The new value of ((1, 10.0, 'L', 'L'), 'S') is  8.64\n",
      "END OF THE EPISODE _______________________________________________\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "State: depth: 0 , Bigger Column: 50 , First column to appear: L\n",
      "current_state setted to None\n",
      "Player looks for best value\n",
      "the agent chose the column that is  B\n",
      "the gain is reduced to 10.0\n",
      "The agent chose the bigger column, , giving a reward of 70.0\n",
      "The current acumulated reward is 70.0\n",
      "Inmediate reward: 70.0\n",
      "update 74.87232 =  74.87232 + 0.8 *( 70.0 + 0.9 * 8.9856 - 74.87232 )\n",
      "The new value of ((0, 50, 'L', 'L'), 'B') is  77.444096\n",
      "\n",
      "\n",
      "State: depth: 1 , Bigger Column: 10.0 , First column to appear: R\n",
      "current_state setted to ((0, 50, 'L', 'L'), 'B')\n",
      "Player looks for best value\n",
      "the agent chose the column that is  S\n",
      "The gain is incremented to 50.0\n",
      "The agent chose the smaller column, giving a reward of 11.0\n",
      "The current acumulated reward is 79.0\n",
      "Inmediate reward: 9.0\n",
      "update 8.9856 =  8.9856 + 0.8 *( 9.0 + 0.9 * 0.0 - 8.9856 )\n",
      "The new value of ((1, 10.0, 'R', 'L'), 'S') is  8.99712\n",
      "END OF THE EPISODE _______________________________________________\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "State: depth: 0 , Bigger Column: 50 , First column to appear: R\n",
      "current_state setted to None\n",
      "Player looks for best value\n",
      "the agent chose the column that is  S\n",
      "The gain is incremented to 250\n",
      "The agent chose the smaller column, giving a reward of 70.0\n",
      "The current acumulated reward is 30.0\n",
      "Inmediate reward: 30.0\n",
      "update 67.96799999999999 =  67.96799999999999 + 0.8 *( 30.0 + 0.9 * 0.0 - 67.96799999999999 )\n",
      "The new value of ((0, 50, 'R', 'R'), 'S') is  37.593599999999995\n",
      "\n",
      "\n",
      "State: depth: 1 , Bigger Column: 250 , First column to appear: R\n",
      "current_state setted to ((0, 50, 'R', 'R'), 'S')\n",
      "Player looks for best value\n",
      "the agent chose the column that is  B\n",
      "the gain is reduced to 50.0\n",
      "The agent chose the bigger column, , giving a reward of 275.0\n",
      "The current acumulated reward is 305.0\n",
      "Inmediate reward: 275.0\n",
      "update 0.0 =  0.0 + 0.8 *( 275.0 + 0.9 * 0.0 - 0.0 )\n",
      "The new value of ((1, 250, 'R', 'R'), 'B') is  220.0\n",
      "END OF THE EPISODE _______________________________________________\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "State: depth: 0 , Bigger Column: 50 , First column to appear: L\n",
      "current_state setted to None\n",
      "Player looks for best value\n",
      "the agent chose the column that is  B\n",
      "the gain is reduced to 10.0\n",
      "The agent chose the bigger column, , giving a reward of 70.0\n",
      "The current acumulated reward is 70.0\n",
      "Inmediate reward: 70.0\n",
      "update 77.444096 =  77.444096 + 0.8 *( 70.0 + 0.9 * 10.56 - 77.444096 )\n",
      "The new value of ((0, 50, 'L', 'L'), 'B') is  79.09201920000001\n",
      "\n",
      "\n",
      "State: depth: 1 , Bigger Column: 10.0 , First column to appear: R\n",
      "current_state setted to ((0, 50, 'L', 'L'), 'B')\n",
      "Player looks for best value\n",
      "the agent chose the column that is  B\n",
      "the gain is reduced to 2.0\n",
      "The agent chose the bigger column, , giving a reward of 11.0\n",
      "The current acumulated reward is 81.0\n",
      "Inmediate reward: 11.0\n",
      "update 10.56 =  10.56 + 0.8 *( 11.0 + 0.9 * 0.0 - 10.56 )\n",
      "The new value of ((1, 10.0, 'R', 'R'), 'B') is  10.912\n",
      "END OF THE EPISODE _______________________________________________\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "State: depth: 0 , Bigger Column: 50 , First column to appear: R\n",
      "current_state setted to None\n",
      "Player looks for best value\n",
      "the agent chose the column that is  S\n",
      "The gain is incremented to 250\n",
      "The agent chose the smaller column, giving a reward of 70.0\n",
      "The current acumulated reward is 30.0\n",
      "Inmediate reward: 30.0\n",
      "update 214.08000000000004 =  214.08000000000004 + 0.8 *( 30.0 + 0.9 * 264.0 - 214.08000000000004 )\n",
      "The new value of ((0, 50, 'R', 'L'), 'S') is  256.896\n",
      "\n",
      "\n",
      "State: depth: 1 , Bigger Column: 250 , First column to appear: L\n",
      "current_state setted to ((0, 50, 'R', 'L'), 'S')\n",
      "Player looks for best value\n",
      "the agent chose the column that is  B\n",
      "the gain is reduced to 50.0\n",
      "The agent chose the bigger column, , giving a reward of 275.0\n",
      "The current acumulated reward is 305.0\n",
      "Inmediate reward: 275.0\n",
      "update 264.0 =  264.0 + 0.8 *( 275.0 + 0.9 * 0.0 - 264.0 )\n",
      "The new value of ((1, 250, 'L', 'L'), 'B') is  272.8\n",
      "END OF THE EPISODE _______________________________________________\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "State: depth: 0 , Bigger Column: 50 , First column to appear: L\n",
      "current_state setted to None\n",
      "Player looks for best value\n",
      "the agent chose the column that is  B\n",
      "the gain is reduced to 10.0\n",
      "The agent chose the bigger column, , giving a reward of 70.0\n",
      "The current acumulated reward is 70.0\n",
      "Inmediate reward: 70.0\n",
      "update 77.37393152 =  77.37393152 + 0.8 *( 70.0 + 0.9 * 10.912 - 77.37393152 )\n",
      "The new value of ((0, 50, 'L', 'R'), 'B') is  79.331426304\n",
      "\n",
      "\n",
      "State: depth: 1 , Bigger Column: 10.0 , First column to appear: L\n",
      "current_state setted to ((0, 50, 'L', 'R'), 'B')\n",
      "Player looks for best value\n",
      "the agent chose the column that is  B\n",
      "the gain is reduced to 2.0\n",
      "The agent chose the bigger column, , giving a reward of 11.0\n",
      "The current acumulated reward is 81.0\n",
      "Inmediate reward: 11.0\n",
      "update 10.912 =  10.912 + 0.8 *( 11.0 + 0.9 * 0.0 - 10.912 )\n",
      "The new value of ((1, 10.0, 'L', 'R'), 'B') is  10.9824\n",
      "END OF THE EPISODE _______________________________________________\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "State: depth: 0 , Bigger Column: 50 , First column to appear: R\n",
      "current_state setted to None\n",
      "exploratory\n",
      "the agent chose the column that is  B\n",
      "the gain is reduced to 10.0\n",
      "The agent chose the bigger column, , giving a reward of 70.0\n",
      "The current acumulated reward is 70.0\n",
      "Inmediate reward: 70.0\n",
      "update 63.60320000000001 =  63.60320000000001 + 0.8 *( 70.0 + 0.9 * 10.912 - 63.60320000000001 )\n",
      "The new value of ((0, 50, 'R', 'R'), 'B') is  76.57728\n",
      "\n",
      "\n",
      "State: depth: 1 , Bigger Column: 10.0 , First column to appear: R\n",
      "current_state setted to ((0, 50, 'R', 'R'), 'B')\n",
      "Player looks for best value\n",
      "the agent chose the column that is  B\n",
      "the gain is reduced to 2.0\n",
      "The agent chose the bigger column, , giving a reward of 11.0\n",
      "The current acumulated reward is 81.0\n",
      "Inmediate reward: 11.0\n",
      "update 10.912 =  10.912 + 0.8 *( 11.0 + 0.9 * 0.0 - 10.912 )\n",
      "The new value of ((1, 10.0, 'R', 'R'), 'B') is  10.9824\n",
      "END OF THE EPISODE _______________________________________________\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "State: depth: 0 , Bigger Column: 50 , First column to appear: L\n",
      "current_state setted to None\n",
      "Player looks for best value\n",
      "the agent chose the column that is  B\n",
      "the gain is reduced to 10.0\n",
      "The agent chose the bigger column, , giving a reward of 70.0\n",
      "The current acumulated reward is 70.0\n",
      "Inmediate reward: 70.0\n",
      "update 79.09201920000001 =  79.09201920000001 + 0.8 *( 70.0 + 0.9 * 8.99712 - 79.09201920000001 )\n",
      "The new value of ((0, 50, 'L', 'L'), 'B') is  78.29633024\n",
      "\n",
      "\n",
      "State: depth: 1 , Bigger Column: 10.0 , First column to appear: R\n",
      "current_state setted to ((0, 50, 'L', 'L'), 'B')\n",
      "exploratory\n",
      "the agent chose the column that is  S\n",
      "The gain is incremented to 50.0\n",
      "The agent chose the smaller column, giving a reward of 11.0\n",
      "The current acumulated reward is 79.0\n",
      "Inmediate reward: 9.0\n",
      "update 8.99712 =  8.99712 + 0.8 *( 9.0 + 0.9 * 0.0 - 8.99712 )\n",
      "The new value of ((1, 10.0, 'R', 'L'), 'S') is  8.999424\n",
      "END OF THE EPISODE _______________________________________________\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "State: depth: 0 , Bigger Column: 50 , First column to appear: R\n",
      "current_state setted to None\n",
      "Player looks for best value\n",
      "the agent chose the column that is  S\n",
      "The gain is incremented to 250\n",
      "The agent chose the smaller column, giving a reward of 70.0\n",
      "The current acumulated reward is 30.0\n",
      "Inmediate reward: 30.0\n",
      "update 256.896 =  256.896 + 0.8 *( 30.0 + 0.9 * 220.0 - 256.896 )\n",
      "The new value of ((0, 50, 'R', 'L'), 'S') is  233.7792\n",
      "\n",
      "\n",
      "State: depth: 1 , Bigger Column: 250 , First column to appear: R\n",
      "current_state setted to ((0, 50, 'R', 'L'), 'S')\n",
      "Player looks for best value\n",
      "the agent chose the column that is  B\n",
      "the gain is reduced to 50.0\n",
      "The agent chose the bigger column, , giving a reward of 275.0\n",
      "The current acumulated reward is 305.0\n",
      "Inmediate reward: 275.0\n",
      "update 220.0 =  220.0 + 0.8 *( 275.0 + 0.9 * 0.0 - 220.0 )\n",
      "The new value of ((1, 250, 'R', 'R'), 'B') is  264.0\n",
      "END OF THE EPISODE _______________________________________________\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "State: depth: 0 , Bigger Column: 50 , First column to appear: R\n",
      "current_state setted to None\n",
      "Player looks for best value\n",
      "the agent chose the column that is  B\n",
      "the gain is reduced to 10.0\n",
      "The agent chose the bigger column, , giving a reward of 70.0\n",
      "The current acumulated reward is 70.0\n",
      "Inmediate reward: 70.0\n",
      "update 76.57728 =  76.57728 + 0.8 *( 70.0 + 0.9 * 8.999424 - 76.57728 )\n",
      "The new value of ((0, 50, 'R', 'R'), 'B') is  77.79504128\n",
      "\n",
      "\n",
      "State: depth: 1 , Bigger Column: 10.0 , First column to appear: R\n",
      "current_state setted to ((0, 50, 'R', 'R'), 'B')\n",
      "exploratory\n",
      "the agent chose the column that is  S\n",
      "The gain is incremented to 50.0\n",
      "The agent chose the smaller column, giving a reward of 11.0\n",
      "The current acumulated reward is 79.0\n",
      "Inmediate reward: 9.0\n",
      "update 8.999424 =  8.999424 + 0.8 *( 9.0 + 0.9 * 0.0 - 8.999424 )\n",
      "The new value of ((1, 10.0, 'R', 'L'), 'S') is  8.9998848\n",
      "END OF THE EPISODE _______________________________________________\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "State: depth: 0 , Bigger Column: 50 , First column to appear: L\n",
      "current_state setted to None\n",
      "Player looks for best value\n",
      "the agent chose the column that is  B\n",
      "the gain is reduced to 10.0\n",
      "The agent chose the bigger column, , giving a reward of 70.0\n",
      "The current acumulated reward is 70.0\n",
      "Inmediate reward: 70.0\n",
      "update 79.331426304 =  79.331426304 + 0.8 *( 70.0 + 0.9 * 10.9824 - 79.331426304 )\n",
      "The new value of ((0, 50, 'L', 'R'), 'B') is  79.77361326079999\n",
      "\n",
      "\n",
      "State: depth: 1 , Bigger Column: 10.0 , First column to appear: L\n",
      "current_state setted to ((0, 50, 'L', 'R'), 'B')\n",
      "Player looks for best value\n",
      "the agent chose the column that is  B\n",
      "the gain is reduced to 2.0\n",
      "The agent chose the bigger column, , giving a reward of 11.0\n",
      "The current acumulated reward is 81.0\n",
      "Inmediate reward: 11.0\n",
      "update 10.9824 =  10.9824 + 0.8 *( 11.0 + 0.9 * 0.0 - 10.9824 )\n",
      "The new value of ((1, 10.0, 'L', 'R'), 'B') is  10.99648\n",
      "END OF THE EPISODE _______________________________________________\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "State: depth: 0 , Bigger Column: 50 , First column to appear: R\n",
      "current_state setted to None\n",
      "Player looks for best value\n",
      "the agent chose the column that is  S\n",
      "The gain is incremented to 250\n",
      "The agent chose the smaller column, giving a reward of 70.0\n",
      "The current acumulated reward is 30.0\n",
      "Inmediate reward: 30.0\n",
      "update 233.7792 =  233.7792 + 0.8 *( 30.0 + 0.9 * 180.0 - 233.7792 )\n",
      "The new value of ((0, 50, 'R', 'L'), 'S') is  200.35584\n",
      "\n",
      "\n",
      "State: depth: 1 , Bigger Column: 250 , First column to appear: L\n",
      "current_state setted to ((0, 50, 'R', 'L'), 'S')\n",
      "Player looks for best value\n",
      "the agent chose the column that is  S\n",
      "The gain is incremented to 1250\n",
      "The agent chose the smaller column, giving a reward of 275.0\n",
      "The current acumulated reward is 255.0\n",
      "Inmediate reward: 225.0\n",
      "update 180.0 =  180.0 + 0.8 *( 225.0 + 0.9 * 0.0 - 180.0 )\n",
      "The new value of ((1, 250, 'L', 'R'), 'S') is  216.0\n",
      "END OF THE EPISODE _______________________________________________\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "State: depth: 0 , Bigger Column: 50 , First column to appear: R\n",
      "current_state setted to None\n",
      "exploratory\n",
      "the agent chose the column that is  S\n",
      "The gain is incremented to 250\n",
      "The agent chose the smaller column, giving a reward of 70.0\n",
      "The current acumulated reward is 30.0\n",
      "Inmediate reward: 30.0\n",
      "update 200.35584 =  200.35584 + 0.8 *( 30.0 + 0.9 * 216.0 - 200.35584 )\n",
      "The new value of ((0, 50, 'R', 'L'), 'S') is  219.591168\n",
      "\n",
      "\n",
      "State: depth: 1 , Bigger Column: 250 , First column to appear: L\n",
      "current_state setted to ((0, 50, 'R', 'L'), 'S')\n",
      "Player looks for best value\n",
      "the agent chose the column that is  S\n",
      "The gain is incremented to 1250\n",
      "The agent chose the smaller column, giving a reward of 275.0\n",
      "The current acumulated reward is 255.0\n",
      "Inmediate reward: 225.0\n",
      "update 216.0 =  216.0 + 0.8 *( 225.0 + 0.9 * 0.0 - 216.0 )\n",
      "The new value of ((1, 250, 'L', 'R'), 'S') is  223.2\n",
      "END OF THE EPISODE _______________________________________________\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "State: depth: 0 , Bigger Column: 50 , First column to appear: R\n",
      "current_state setted to None\n",
      "Player looks for best value\n",
      "the agent chose the column that is  B\n",
      "the gain is reduced to 10.0\n",
      "The agent chose the bigger column, , giving a reward of 70.0\n",
      "The current acumulated reward is 70.0\n",
      "Inmediate reward: 70.0\n",
      "update 77.79504128 =  77.79504128 + 0.8 *( 70.0 + 0.9 * 8.9998848 - 77.79504128 )\n",
      "The new value of ((0, 50, 'R', 'R'), 'B') is  78.038925312\n",
      "\n",
      "\n",
      "State: depth: 1 , Bigger Column: 10.0 , First column to appear: R\n",
      "current_state setted to ((0, 50, 'R', 'R'), 'B')\n",
      "Player looks for best value\n",
      "the agent chose the column that is  S\n",
      "The gain is incremented to 50.0\n",
      "The agent chose the smaller column, giving a reward of 11.0\n",
      "The current acumulated reward is 79.0\n",
      "Inmediate reward: 9.0\n",
      "update 8.9998848 =  8.9998848 + 0.8 *( 9.0 + 0.9 * 0.0 - 8.9998848 )\n",
      "The new value of ((1, 10.0, 'R', 'L'), 'S') is  8.99997696\n",
      "END OF THE EPISODE _______________________________________________\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "State: depth: 0 , Bigger Column: 50 , First column to appear: R\n",
      "current_state setted to None\n",
      "Player looks for best value\n",
      "the agent chose the column that is  B\n",
      "the gain is reduced to 10.0\n",
      "The agent chose the bigger column, , giving a reward of 70.0\n",
      "The current acumulated reward is 70.0\n",
      "Inmediate reward: 70.0\n",
      "update 78.038925312 =  78.038925312 + 0.8 *( 70.0 + 0.9 * 8.99997696 - 78.038925312 )\n",
      "The new value of ((0, 50, 'R', 'R'), 'B') is  78.0877684736\n",
      "\n",
      "\n",
      "State: depth: 1 , Bigger Column: 10.0 , First column to appear: R\n",
      "current_state setted to ((0, 50, 'R', 'R'), 'B')\n",
      "Player looks for best value\n",
      "the agent chose the column that is  S\n",
      "The gain is incremented to 50.0\n",
      "The agent chose the smaller column, giving a reward of 11.0\n",
      "The current acumulated reward is 79.0\n",
      "Inmediate reward: 9.0\n",
      "update 8.99997696 =  8.99997696 + 0.8 *( 9.0 + 0.9 * 0.0 - 8.99997696 )\n",
      "The new value of ((1, 10.0, 'R', 'L'), 'S') is  8.999995392\n",
      "END OF THE EPISODE _______________________________________________\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "State: depth: 0 , Bigger Column: 50 , First column to appear: R\n",
      "current_state setted to None\n",
      "Player looks for best value\n",
      "the agent chose the column that is  S\n",
      "The gain is incremented to 250\n",
      "The agent chose the smaller column, giving a reward of 70.0\n",
      "The current acumulated reward is 30.0\n",
      "Inmediate reward: 30.0\n",
      "update 219.591168 =  219.591168 + 0.8 *( 30.0 + 0.9 * 272.8 - 219.591168 )\n",
      "The new value of ((0, 50, 'R', 'L'), 'S') is  264.3342336\n",
      "\n",
      "\n",
      "State: depth: 1 , Bigger Column: 250 , First column to appear: L\n",
      "current_state setted to ((0, 50, 'R', 'L'), 'S')\n",
      "Player looks for best value\n",
      "the agent chose the column that is  B\n",
      "the gain is reduced to 50.0\n",
      "The agent chose the bigger column, , giving a reward of 275.0\n",
      "The current acumulated reward is 305.0\n",
      "Inmediate reward: 275.0\n",
      "update 272.8 =  272.8 + 0.8 *( 275.0 + 0.9 * 0.0 - 272.8 )\n",
      "The new value of ((1, 250, 'L', 'L'), 'B') is  274.56\n",
      "END OF THE EPISODE _______________________________________________\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "State: depth: 0 , Bigger Column: 50 , First column to appear: L\n",
      "current_state setted to None\n",
      "Player looks for best value\n",
      "the agent chose the column that is  B\n",
      "the gain is reduced to 10.0\n",
      "The agent chose the bigger column, , giving a reward of 70.0\n",
      "The current acumulated reward is 70.0\n",
      "Inmediate reward: 70.0\n",
      "update 79.77361326079999 =  79.77361326079999 + 0.8 *( 70.0 + 0.9 * 10.99648 - 79.77361326079999 )\n",
      "The new value of ((0, 50, 'L', 'R'), 'B') is  79.87218825216\n",
      "\n",
      "\n",
      "State: depth: 1 , Bigger Column: 10.0 , First column to appear: L\n",
      "current_state setted to ((0, 50, 'L', 'R'), 'B')\n",
      "Player looks for best value\n",
      "the agent chose the column that is  B\n",
      "the gain is reduced to 2.0\n",
      "The agent chose the bigger column, , giving a reward of 11.0\n",
      "The current acumulated reward is 81.0\n",
      "Inmediate reward: 11.0\n",
      "update 10.99648 =  10.99648 + 0.8 *( 11.0 + 0.9 * 0.0 - 10.99648 )\n",
      "The new value of ((1, 10.0, 'L', 'R'), 'B') is  10.999296\n",
      "END OF THE EPISODE _______________________________________________\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "State: depth: 0 , Bigger Column: 50 , First column to appear: L\n",
      "current_state setted to None\n",
      "exploratory\n",
      "the agent chose the column that is  B\n",
      "the gain is reduced to 10.0\n",
      "The agent chose the bigger column, , giving a reward of 70.0\n",
      "The current acumulated reward is 70.0\n",
      "Inmediate reward: 70.0\n",
      "update 78.29633024 =  78.29633024 + 0.8 *( 70.0 + 0.9 * 8.999995392 - 78.29633024 )\n",
      "The new value of ((0, 50, 'L', 'L'), 'B') is  78.13926273024\n",
      "\n",
      "\n",
      "State: depth: 1 , Bigger Column: 10.0 , First column to appear: R\n",
      "current_state setted to ((0, 50, 'L', 'L'), 'B')\n",
      "Player looks for best value\n",
      "the agent chose the column that is  S\n",
      "The gain is incremented to 50.0\n",
      "The agent chose the smaller column, giving a reward of 11.0\n",
      "The current acumulated reward is 79.0\n",
      "Inmediate reward: 9.0\n",
      "update 8.999995392 =  8.999995392 + 0.8 *( 9.0 + 0.9 * 0.0 - 8.999995392 )\n",
      "The new value of ((1, 10.0, 'R', 'L'), 'S') is  8.9999990784\n",
      "END OF THE EPISODE _______________________________________________\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "State: depth: 0 , Bigger Column: 50 , First column to appear: L\n",
      "current_state setted to None\n",
      "exploratory\n",
      "the agent chose the column that is  S\n",
      "The gain is incremented to 250\n",
      "The agent chose the smaller column, giving a reward of 70.0\n",
      "The current acumulated reward is 30.0\n",
      "Inmediate reward: 30.0\n",
      "update 24.0 =  24.0 + 0.8 *( 30.0 + 0.9 * 272.8 - 24.0 )\n",
      "The new value of ((0, 50, 'L', 'R'), 'S') is  225.216\n",
      "\n",
      "\n",
      "State: depth: 1 , Bigger Column: 250 , First column to appear: R\n",
      "current_state setted to ((0, 50, 'L', 'R'), 'S')\n",
      "exploratory\n",
      "the agent chose the column that is  B\n",
      "the gain is reduced to 50.0\n",
      "The agent chose the bigger column, , giving a reward of 275.0\n",
      "The current acumulated reward is 305.0\n",
      "Inmediate reward: 275.0\n",
      "update 272.8 =  272.8 + 0.8 *( 275.0 + 0.9 * 0.0 - 272.8 )\n",
      "The new value of ((1, 250, 'R', 'L'), 'B') is  274.56\n",
      "END OF THE EPISODE _______________________________________________\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "State: depth: 0 , Bigger Column: 50 , First column to appear: L\n",
      "current_state setted to None\n",
      "Player looks for best value\n",
      "the agent chose the column that is  S\n",
      "The gain is incremented to 250\n",
      "The agent chose the smaller column, giving a reward of 70.0\n",
      "The current acumulated reward is 30.0\n",
      "Inmediate reward: 30.0\n",
      "update 225.216 =  225.216 + 0.8 *( 30.0 + 0.9 * 223.2 - 225.216 )\n",
      "The new value of ((0, 50, 'L', 'R'), 'S') is  229.7472\n",
      "\n",
      "\n",
      "State: depth: 1 , Bigger Column: 250 , First column to appear: L\n",
      "current_state setted to ((0, 50, 'L', 'R'), 'S')\n",
      "Player looks for best value\n",
      "the agent chose the column that is  S\n",
      "The gain is incremented to 1250\n",
      "The agent chose the smaller column, giving a reward of 275.0\n",
      "The current acumulated reward is 255.0\n",
      "Inmediate reward: 225.0\n",
      "update 223.2 =  223.2 + 0.8 *( 225.0 + 0.9 * 0.0 - 223.2 )\n",
      "The new value of ((1, 250, 'L', 'R'), 'S') is  224.64\n",
      "END OF THE EPISODE _______________________________________________\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "State: depth: 0 , Bigger Column: 50 , First column to appear: R\n",
      "current_state setted to None\n",
      "exploratory\n",
      "the agent chose the column that is  B\n",
      "the gain is reduced to 10.0\n",
      "The agent chose the bigger column, , giving a reward of 70.0\n",
      "The current acumulated reward is 70.0\n",
      "Inmediate reward: 70.0\n",
      "update 78.0877684736 =  78.0877684736 + 0.8 *( 70.0 + 0.9 * 8.9999990784 - 78.0877684736 )\n",
      "The new value of ((0, 50, 'R', 'R'), 'B') is  78.09755303116799\n",
      "\n",
      "\n",
      "State: depth: 1 , Bigger Column: 10.0 , First column to appear: R\n",
      "current_state setted to ((0, 50, 'R', 'R'), 'B')\n",
      "Player looks for best value\n",
      "the agent chose the column that is  S\n",
      "The gain is incremented to 50.0\n",
      "The agent chose the smaller column, giving a reward of 11.0\n",
      "The current acumulated reward is 79.0\n",
      "Inmediate reward: 9.0\n",
      "update 8.9999990784 =  8.9999990784 + 0.8 *( 9.0 + 0.9 * 0.0 - 8.9999990784 )\n",
      "The new value of ((1, 10.0, 'R', 'L'), 'S') is  8.99999981568\n",
      "END OF THE EPISODE _______________________________________________\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "State: depth: 0 , Bigger Column: 50 , First column to appear: L\n",
      "current_state setted to None\n",
      "Player looks for best value\n",
      "the agent chose the column that is  S\n",
      "The gain is incremented to 250\n",
      "The agent chose the smaller column, giving a reward of 70.0\n",
      "The current acumulated reward is 30.0\n",
      "Inmediate reward: 30.0\n",
      "update 229.7472 =  229.7472 + 0.8 *( 30.0 + 0.9 * 224.64 - 229.7472 )\n",
      "The new value of ((0, 50, 'L', 'R'), 'S') is  231.69024\n",
      "\n",
      "\n",
      "State: depth: 1 , Bigger Column: 250 , First column to appear: L\n",
      "current_state setted to ((0, 50, 'L', 'R'), 'S')\n",
      "Player looks for best value\n",
      "the agent chose the column that is  S\n",
      "The gain is incremented to 1250\n",
      "The agent chose the smaller column, giving a reward of 275.0\n",
      "The current acumulated reward is 255.0\n",
      "Inmediate reward: 225.0\n",
      "update 224.64 =  224.64 + 0.8 *( 225.0 + 0.9 * 0.0 - 224.64 )\n",
      "The new value of ((1, 250, 'L', 'R'), 'S') is  224.928\n",
      "END OF THE EPISODE _______________________________________________\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "State: depth: 0 , Bigger Column: 50 , First column to appear: R\n",
      "current_state setted to None\n",
      "Player looks for best value\n",
      "the agent chose the column that is  B\n",
      "the gain is reduced to 10.0\n",
      "The agent chose the bigger column, , giving a reward of 70.0\n",
      "The current acumulated reward is 70.0\n",
      "Inmediate reward: 70.0\n",
      "update 78.09755303116799 =  78.09755303116799 + 0.8 *( 70.0 + 0.9 * 10.9824 - 78.09755303116799 )\n",
      "The new value of ((0, 50, 'R', 'R'), 'B') is  79.5268386062336\n",
      "\n",
      "\n",
      "State: depth: 1 , Bigger Column: 10.0 , First column to appear: R\n",
      "current_state setted to ((0, 50, 'R', 'R'), 'B')\n",
      "Player looks for best value\n",
      "the agent chose the column that is  B\n",
      "the gain is reduced to 2.0\n",
      "The agent chose the bigger column, , giving a reward of 11.0\n",
      "The current acumulated reward is 81.0\n",
      "Inmediate reward: 11.0\n",
      "update 10.9824 =  10.9824 + 0.8 *( 11.0 + 0.9 * 0.0 - 10.9824 )\n",
      "The new value of ((1, 10.0, 'R', 'R'), 'B') is  10.99648\n",
      "END OF THE EPISODE _______________________________________________\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "State: depth: 0 , Bigger Column: 50 , First column to appear: L\n",
      "current_state setted to None\n",
      "Player looks for best value\n",
      "the agent chose the column that is  B\n",
      "the gain is reduced to 10.0\n",
      "The agent chose the bigger column, , giving a reward of 70.0\n",
      "The current acumulated reward is 70.0\n",
      "Inmediate reward: 70.0\n",
      "update 78.13926273024 =  78.13926273024 + 0.8 *( 70.0 + 0.9 * 10.99648 - 78.13926273024 )\n",
      "The new value of ((0, 50, 'L', 'L'), 'B') is  79.545318146048\n",
      "\n",
      "\n",
      "State: depth: 1 , Bigger Column: 10.0 , First column to appear: R\n",
      "current_state setted to ((0, 50, 'L', 'L'), 'B')\n",
      "Player looks for best value\n",
      "the agent chose the column that is  B\n",
      "the gain is reduced to 2.0\n",
      "The agent chose the bigger column, , giving a reward of 11.0\n",
      "The current acumulated reward is 81.0\n",
      "Inmediate reward: 11.0\n",
      "update 10.99648 =  10.99648 + 0.8 *( 11.0 + 0.9 * 0.0 - 10.99648 )\n",
      "The new value of ((1, 10.0, 'R', 'R'), 'B') is  10.999296\n",
      "END OF THE EPISODE _______________________________________________\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "State: depth: 0 , Bigger Column: 50 , First column to appear: R\n",
      "current_state setted to None\n",
      "Player looks for best value\n",
      "the agent chose the column that is  B\n",
      "the gain is reduced to 10.0\n",
      "The agent chose the bigger column, , giving a reward of 70.0\n",
      "The current acumulated reward is 70.0\n",
      "Inmediate reward: 70.0\n",
      "update 79.5268386062336 =  79.5268386062336 + 0.8 *( 70.0 + 0.9 * 8.99999981568 - 79.5268386062336 )\n",
      "The new value of ((0, 50, 'R', 'R'), 'B') is  78.38536758853631\n",
      "\n",
      "\n",
      "State: depth: 1 , Bigger Column: 10.0 , First column to appear: R\n",
      "current_state setted to ((0, 50, 'R', 'R'), 'B')\n",
      "exploratory\n",
      "the agent chose the column that is  S\n",
      "The gain is incremented to 50.0\n",
      "The agent chose the smaller column, giving a reward of 11.0\n",
      "The current acumulated reward is 79.0\n",
      "Inmediate reward: 9.0\n",
      "update 8.99999981568 =  8.99999981568 + 0.8 *( 9.0 + 0.9 * 0.0 - 8.99999981568 )\n",
      "The new value of ((1, 10.0, 'R', 'L'), 'S') is  8.999999963136\n",
      "END OF THE EPISODE _______________________________________________\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "State: depth: 0 , Bigger Column: 50 , First column to appear: R\n",
      "current_state setted to None\n",
      "Player looks for best value\n",
      "the agent chose the column that is  B\n",
      "the gain is reduced to 10.0\n",
      "The agent chose the bigger column, , giving a reward of 70.0\n",
      "The current acumulated reward is 70.0\n",
      "Inmediate reward: 70.0\n",
      "update 78.38536758853631 =  78.38536758853631 + 0.8 *( 70.0 + 0.9 * 10.999296 - 78.38536758853631 )\n",
      "The new value of ((0, 50, 'R', 'R'), 'B') is  79.59656663770727\n",
      "\n",
      "\n",
      "State: depth: 1 , Bigger Column: 10.0 , First column to appear: L\n",
      "current_state setted to ((0, 50, 'R', 'R'), 'B')\n",
      "Player looks for best value\n",
      "the agent chose the column that is  B\n",
      "the gain is reduced to 2.0\n",
      "The agent chose the bigger column, , giving a reward of 11.0\n",
      "The current acumulated reward is 81.0\n",
      "Inmediate reward: 11.0\n",
      "update 10.999296 =  10.999296 + 0.8 *( 11.0 + 0.9 * 0.0 - 10.999296 )\n",
      "The new value of ((1, 10.0, 'L', 'R'), 'B') is  10.9998592\n",
      "END OF THE EPISODE _______________________________________________\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "State: depth: 0 , Bigger Column: 50 , First column to appear: R\n",
      "current_state setted to None\n",
      "Player looks for best value\n",
      "the agent chose the column that is  B\n",
      "the gain is reduced to 10.0\n",
      "The agent chose the bigger column, , giving a reward of 70.0\n",
      "The current acumulated reward is 70.0\n",
      "Inmediate reward: 70.0\n",
      "update 79.59656663770727 =  79.59656663770727 + 0.8 *( 70.0 + 0.9 * 10.999296 - 79.59656663770727 )\n",
      "The new value of ((0, 50, 'R', 'R'), 'B') is  79.83880644754146\n",
      "\n",
      "\n",
      "State: depth: 1 , Bigger Column: 10.0 , First column to appear: R\n",
      "current_state setted to ((0, 50, 'R', 'R'), 'B')\n",
      "Player looks for best value\n",
      "the agent chose the column that is  B\n",
      "the gain is reduced to 2.0\n",
      "The agent chose the bigger column, , giving a reward of 11.0\n",
      "The current acumulated reward is 81.0\n",
      "Inmediate reward: 11.0\n",
      "update 10.999296 =  10.999296 + 0.8 *( 11.0 + 0.9 * 0.0 - 10.999296 )\n",
      "The new value of ((1, 10.0, 'R', 'R'), 'B') is  10.9998592\n",
      "END OF THE EPISODE _______________________________________________\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "State: depth: 0 , Bigger Column: 50 , First column to appear: L\n",
      "current_state setted to None\n",
      "Player looks for best value\n",
      "the agent chose the column that is  S\n",
      "The gain is incremented to 250\n",
      "The agent chose the smaller column, giving a reward of 70.0\n",
      "The current acumulated reward is 30.0\n",
      "Inmediate reward: 30.0\n",
      "update 231.69024 =  231.69024 + 0.8 *( 30.0 + 0.9 * 264.0 - 231.69024 )\n",
      "The new value of ((0, 50, 'L', 'R'), 'S') is  260.418048\n",
      "\n",
      "\n",
      "State: depth: 1 , Bigger Column: 250 , First column to appear: R\n",
      "current_state setted to ((0, 50, 'L', 'R'), 'S')\n",
      "Player looks for best value\n",
      "the agent chose the column that is  B\n",
      "the gain is reduced to 50.0\n",
      "The agent chose the bigger column, , giving a reward of 275.0\n",
      "The current acumulated reward is 305.0\n",
      "Inmediate reward: 275.0\n",
      "update 264.0 =  264.0 + 0.8 *( 275.0 + 0.9 * 0.0 - 264.0 )\n",
      "The new value of ((1, 250, 'R', 'R'), 'B') is  272.8\n",
      "END OF THE EPISODE _______________________________________________\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "State: depth: 0 , Bigger Column: 50 , First column to appear: L\n",
      "current_state setted to None\n",
      "Player looks for best value\n",
      "the agent chose the column that is  S\n",
      "The gain is incremented to 250\n",
      "The agent chose the smaller column, giving a reward of 70.0\n",
      "The current acumulated reward is 30.0\n",
      "Inmediate reward: 30.0\n",
      "update 260.418048 =  260.418048 + 0.8 *( 30.0 + 0.9 * 274.56 - 260.418048 )\n",
      "The new value of ((0, 50, 'L', 'R'), 'S') is  273.76680960000004\n",
      "\n",
      "\n",
      "State: depth: 1 , Bigger Column: 250 , First column to appear: R\n",
      "current_state setted to ((0, 50, 'L', 'R'), 'S')\n",
      "Player looks for best value\n",
      "the agent chose the column that is  B\n",
      "the gain is reduced to 50.0\n",
      "The agent chose the bigger column, , giving a reward of 275.0\n",
      "The current acumulated reward is 305.0\n",
      "Inmediate reward: 275.0\n",
      "update 274.56 =  274.56 + 0.8 *( 275.0 + 0.9 * 0.0 - 274.56 )\n",
      "The new value of ((1, 250, 'R', 'L'), 'B') is  274.912\n",
      "END OF THE EPISODE _______________________________________________\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "State: depth: 0 , Bigger Column: 50 , First column to appear: L\n",
      "current_state setted to None\n",
      "Player looks for best value\n",
      "the agent chose the column that is  B\n",
      "the gain is reduced to 10.0\n",
      "The agent chose the bigger column, , giving a reward of 70.0\n",
      "The current acumulated reward is 70.0\n",
      "Inmediate reward: 70.0\n",
      "update 79.545318146048 =  79.545318146048 + 0.8 *( 70.0 + 0.9 * 10.9998592 - 79.545318146048 )\n",
      "The new value of ((0, 50, 'L', 'L'), 'B') is  79.8289622532096\n",
      "\n",
      "\n",
      "State: depth: 1 , Bigger Column: 10.0 , First column to appear: L\n",
      "current_state setted to ((0, 50, 'L', 'L'), 'B')\n",
      "Player looks for best value\n",
      "the agent chose the column that is  B\n",
      "the gain is reduced to 2.0\n",
      "The agent chose the bigger column, , giving a reward of 11.0\n",
      "The current acumulated reward is 81.0\n",
      "Inmediate reward: 11.0\n",
      "update 10.9998592 =  10.9998592 + 0.8 *( 11.0 + 0.9 * 0.0 - 10.9998592 )\n",
      "The new value of ((1, 10.0, 'L', 'R'), 'B') is  10.99997184\n",
      "END OF THE EPISODE _______________________________________________\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "State: depth: 0 , Bigger Column: 50 , First column to appear: R\n",
      "current_state setted to None\n",
      "Player looks for best value\n",
      "the agent chose the column that is  S\n",
      "The gain is incremented to 250\n",
      "The agent chose the smaller column, giving a reward of 70.0\n",
      "The current acumulated reward is 30.0\n",
      "Inmediate reward: 30.0\n",
      "update 264.3342336 =  264.3342336 + 0.8 *( 30.0 + 0.9 * 274.912 - 264.3342336 )\n",
      "The new value of ((0, 50, 'R', 'L'), 'S') is  274.80348671999997\n",
      "\n",
      "\n",
      "State: depth: 1 , Bigger Column: 250 , First column to appear: R\n",
      "current_state setted to ((0, 50, 'R', 'L'), 'S')\n",
      "Player looks for best value\n",
      "the agent chose the column that is  B\n",
      "the gain is reduced to 50.0\n",
      "The agent chose the bigger column, , giving a reward of 275.0\n",
      "The current acumulated reward is 305.0\n",
      "Inmediate reward: 275.0\n",
      "update 274.912 =  274.912 + 0.8 *( 275.0 + 0.9 * 0.0 - 274.912 )\n",
      "The new value of ((1, 250, 'R', 'L'), 'B') is  274.9824\n",
      "END OF THE EPISODE _______________________________________________\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "State: depth: 0 , Bigger Column: 50 , First column to appear: R\n",
      "current_state setted to None\n",
      "Player looks for best value\n",
      "the agent chose the column that is  B\n",
      "the gain is reduced to 10.0\n",
      "The agent chose the bigger column, , giving a reward of 70.0\n",
      "The current acumulated reward is 70.0\n",
      "Inmediate reward: 70.0\n",
      "update 79.83880644754146 =  79.83880644754146 + 0.8 *( 70.0 + 0.9 * 8.999999963136 - 79.83880644754146 )\n",
      "The new value of ((0, 50, 'R', 'R'), 'B') is  78.44776126296621\n",
      "\n",
      "\n",
      "State: depth: 1 , Bigger Column: 10.0 , First column to appear: R\n",
      "current_state setted to ((0, 50, 'R', 'R'), 'B')\n",
      "exploratory\n",
      "the agent chose the column that is  B\n",
      "the gain is reduced to 2.0\n",
      "The agent chose the bigger column, , giving a reward of 11.0\n",
      "The current acumulated reward is 81.0\n",
      "Inmediate reward: 11.0\n",
      "update 0.0 =  0.0 + 0.8 *( 11.0 + 0.9 * 0.0 - 0.0 )\n",
      "The new value of ((1, 10.0, 'R', 'L'), 'B') is  8.8\n",
      "END OF THE EPISODE _______________________________________________\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "State: depth: 0 , Bigger Column: 50 , First column to appear: L\n",
      "current_state setted to None\n",
      "Player looks for best value\n",
      "the agent chose the column that is  B\n",
      "the gain is reduced to 10.0\n",
      "The agent chose the bigger column, , giving a reward of 70.0\n",
      "The current acumulated reward is 70.0\n",
      "Inmediate reward: 70.0\n",
      "update 79.8289622532096 =  79.8289622532096 + 0.8 *( 70.0 + 0.9 * 8.64 - 79.8289622532096 )\n",
      "The new value of ((0, 50, 'L', 'L'), 'B') is  78.18659245064191\n",
      "\n",
      "\n",
      "State: depth: 1 , Bigger Column: 10.0 , First column to appear: L\n",
      "current_state setted to ((0, 50, 'L', 'L'), 'B')\n",
      "Player looks for best value\n",
      "the agent chose the column that is  S\n",
      "The gain is incremented to 50.0\n",
      "The agent chose the smaller column, giving a reward of 11.0\n",
      "The current acumulated reward is 79.0\n",
      "Inmediate reward: 9.0\n",
      "update 8.64 =  8.64 + 0.8 *( 9.0 + 0.9 * 0.0 - 8.64 )\n",
      "The new value of ((1, 10.0, 'L', 'L'), 'S') is  8.928\n",
      "END OF THE EPISODE _______________________________________________\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "State: depth: 0 , Bigger Column: 50 , First column to appear: R\n",
      "current_state setted to None\n",
      "Player looks for best value\n",
      "the agent chose the column that is  S\n",
      "The gain is incremented to 250\n",
      "The agent chose the smaller column, giving a reward of 70.0\n",
      "The current acumulated reward is 30.0\n",
      "Inmediate reward: 30.0\n",
      "update 274.80348671999997 =  274.80348671999997 + 0.8 *( 30.0 + 0.9 * 274.9824 - 274.80348671999997 )\n",
      "The new value of ((0, 50, 'R', 'L'), 'S') is  276.948025344\n",
      "\n",
      "\n",
      "State: depth: 1 , Bigger Column: 250 , First column to appear: R\n",
      "current_state setted to ((0, 50, 'R', 'L'), 'S')\n",
      "Player looks for best value\n",
      "the agent chose the column that is  B\n",
      "the gain is reduced to 50.0\n",
      "The agent chose the bigger column, , giving a reward of 275.0\n",
      "The current acumulated reward is 305.0\n",
      "Inmediate reward: 275.0\n",
      "update 274.9824 =  274.9824 + 0.8 *( 275.0 + 0.9 * 0.0 - 274.9824 )\n",
      "The new value of ((1, 250, 'R', 'L'), 'B') is  274.99648\n",
      "END OF THE EPISODE _______________________________________________\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "State: depth: 0 , Bigger Column: 50 , First column to appear: R\n",
      "current_state setted to None\n",
      "Player looks for best value\n",
      "the agent chose the column that is  S\n",
      "The gain is incremented to 250\n",
      "The agent chose the smaller column, giving a reward of 70.0\n",
      "The current acumulated reward is 30.0\n",
      "Inmediate reward: 30.0\n",
      "update 276.948025344 =  276.948025344 + 0.8 *( 30.0 + 0.9 * 272.8 - 276.948025344 )\n",
      "The new value of ((0, 50, 'R', 'L'), 'S') is  275.8056050688\n",
      "\n",
      "\n",
      "State: depth: 1 , Bigger Column: 250 , First column to appear: R\n",
      "current_state setted to ((0, 50, 'R', 'L'), 'S')\n",
      "exploratory\n",
      "the agent chose the column that is  B\n",
      "the gain is reduced to 50.0\n",
      "The agent chose the bigger column, , giving a reward of 275.0\n",
      "The current acumulated reward is 305.0\n",
      "Inmediate reward: 275.0\n",
      "update 272.8 =  272.8 + 0.8 *( 275.0 + 0.9 * 0.0 - 272.8 )\n",
      "The new value of ((1, 250, 'R', 'R'), 'B') is  274.56\n",
      "END OF THE EPISODE _______________________________________________\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "State: depth: 0 , Bigger Column: 50 , First column to appear: L\n",
      "current_state setted to None\n",
      "exploratory\n",
      "the agent chose the column that is  S\n",
      "The gain is incremented to 250\n",
      "The agent chose the smaller column, giving a reward of 70.0\n",
      "The current acumulated reward is 30.0\n",
      "Inmediate reward: 30.0\n",
      "update 273.76680960000004 =  273.76680960000004 + 0.8 *( 30.0 + 0.9 * 274.56 - 273.76680960000004 )\n",
      "The new value of ((0, 50, 'L', 'R'), 'S') is  276.43656192000003\n",
      "\n",
      "\n",
      "State: depth: 1 , Bigger Column: 250 , First column to appear: R\n",
      "current_state setted to ((0, 50, 'L', 'R'), 'S')\n",
      "Player looks for best value\n",
      "the agent chose the column that is  B\n",
      "the gain is reduced to 50.0\n",
      "The agent chose the bigger column, , giving a reward of 275.0\n",
      "The current acumulated reward is 305.0\n",
      "Inmediate reward: 275.0\n",
      "update 274.56 =  274.56 + 0.8 *( 275.0 + 0.9 * 0.0 - 274.56 )\n",
      "The new value of ((1, 250, 'R', 'R'), 'B') is  274.912\n",
      "END OF THE EPISODE _______________________________________________\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "State: depth: 0 , Bigger Column: 50 , First column to appear: R\n",
      "current_state setted to None\n",
      "Player looks for best value\n",
      "the agent chose the column that is  B\n",
      "the gain is reduced to 10.0\n",
      "The agent chose the bigger column, , giving a reward of 70.0\n",
      "The current acumulated reward is 70.0\n",
      "Inmediate reward: 70.0\n",
      "update 78.44776126296621 =  78.44776126296621 + 0.8 *( 70.0 + 0.9 * 8.999999963136 - 78.44776126296621 )\n",
      "The new value of ((0, 50, 'R', 'R'), 'B') is  78.16955222605115\n",
      "\n",
      "\n",
      "State: depth: 1 , Bigger Column: 10.0 , First column to appear: R\n",
      "current_state setted to ((0, 50, 'R', 'R'), 'B')\n",
      "Player looks for best value\n",
      "the agent chose the column that is  S\n",
      "The gain is incremented to 50.0\n",
      "The agent chose the smaller column, giving a reward of 11.0\n",
      "The current acumulated reward is 79.0\n",
      "Inmediate reward: 9.0\n",
      "update 8.999999963136 =  8.999999963136 + 0.8 *( 9.0 + 0.9 * 0.0 - 8.999999963136 )\n",
      "The new value of ((1, 10.0, 'R', 'L'), 'S') is  8.9999999926272\n",
      "END OF THE EPISODE _______________________________________________\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "State: depth: 0 , Bigger Column: 50 , First column to appear: L\n",
      "current_state setted to None\n",
      "Player looks for best value\n",
      "the agent chose the column that is  B\n",
      "the gain is reduced to 10.0\n",
      "The agent chose the bigger column, , giving a reward of 70.0\n",
      "The current acumulated reward is 70.0\n",
      "Inmediate reward: 70.0\n",
      "update 78.18659245064191 =  78.18659245064191 + 0.8 *( 70.0 + 0.9 * 8.9999999926272 - 78.18659245064191 )\n",
      "The new value of ((0, 50, 'L', 'L'), 'B') is  78.11731848481996\n",
      "\n",
      "\n",
      "State: depth: 1 , Bigger Column: 10.0 , First column to appear: R\n",
      "current_state setted to ((0, 50, 'L', 'L'), 'B')\n",
      "exploratory\n",
      "the agent chose the column that is  B\n",
      "the gain is reduced to 2.0\n",
      "The agent chose the bigger column, , giving a reward of 11.0\n",
      "The current acumulated reward is 81.0\n",
      "Inmediate reward: 11.0\n",
      "update 8.8 =  8.8 + 0.8 *( 11.0 + 0.9 * 0.0 - 8.8 )\n",
      "The new value of ((1, 10.0, 'R', 'L'), 'B') is  10.56\n",
      "END OF THE EPISODE _______________________________________________\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "State: depth: 0 , Bigger Column: 50 , First column to appear: R\n",
      "current_state setted to None\n",
      "Player looks for best value\n",
      "the agent chose the column that is  S\n",
      "The gain is incremented to 250\n",
      "The agent chose the smaller column, giving a reward of 70.0\n",
      "The current acumulated reward is 30.0\n",
      "Inmediate reward: 30.0\n",
      "update 275.8056050688 =  275.8056050688 + 0.8 *( 30.0 + 0.9 * 224.928 - 275.8056050688 )\n",
      "The new value of ((0, 50, 'R', 'L'), 'S') is  241.10928101376\n",
      "\n",
      "\n",
      "State: depth: 1 , Bigger Column: 250 , First column to appear: L\n",
      "current_state setted to ((0, 50, 'R', 'L'), 'S')\n",
      "Player looks for best value\n",
      "the agent chose the column that is  S\n",
      "The gain is incremented to 1250\n",
      "The agent chose the smaller column, giving a reward of 275.0\n",
      "The current acumulated reward is 255.0\n",
      "Inmediate reward: 225.0\n",
      "update 224.928 =  224.928 + 0.8 *( 225.0 + 0.9 * 0.0 - 224.928 )\n",
      "The new value of ((1, 250, 'L', 'R'), 'S') is  224.9856\n",
      "END OF THE EPISODE _______________________________________________\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "State: depth: 0 , Bigger Column: 50 , First column to appear: L\n",
      "current_state setted to None\n",
      "Player looks for best value\n",
      "the agent chose the column that is  S\n",
      "The gain is incremented to 250\n",
      "The agent chose the smaller column, giving a reward of 70.0\n",
      "The current acumulated reward is 30.0\n",
      "Inmediate reward: 30.0\n",
      "update 276.43656192000003 =  276.43656192000003 + 0.8 *( 30.0 + 0.9 * 274.912 - 276.43656192000003 )\n",
      "The new value of ((0, 50, 'L', 'R'), 'S') is  277.223952384\n",
      "\n",
      "\n",
      "State: depth: 1 , Bigger Column: 250 , First column to appear: R\n",
      "current_state setted to ((0, 50, 'L', 'R'), 'S')\n",
      "Player looks for best value\n",
      "the agent chose the column that is  B\n",
      "the gain is reduced to 50.0\n",
      "The agent chose the bigger column, , giving a reward of 275.0\n",
      "The current acumulated reward is 305.0\n",
      "Inmediate reward: 275.0\n",
      "update 274.912 =  274.912 + 0.8 *( 275.0 + 0.9 * 0.0 - 274.912 )\n",
      "The new value of ((1, 250, 'R', 'R'), 'B') is  274.9824\n",
      "END OF THE EPISODE _______________________________________________\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "State: depth: 0 , Bigger Column: 50 , First column to appear: L\n",
      "current_state setted to None\n",
      "Player looks for best value\n",
      "the agent chose the column that is  B\n",
      "the gain is reduced to 10.0\n",
      "The agent chose the bigger column, , giving a reward of 70.0\n",
      "The current acumulated reward is 70.0\n",
      "Inmediate reward: 70.0\n",
      "update 78.11731848481996 =  78.11731848481996 + 0.8 *( 70.0 + 0.9 * 8.928 - 78.11731848481996 )\n",
      "The new value of ((0, 50, 'L', 'L'), 'B') is  78.051623696964\n",
      "\n",
      "\n",
      "State: depth: 1 , Bigger Column: 10.0 , First column to appear: L\n",
      "current_state setted to ((0, 50, 'L', 'L'), 'B')\n",
      "exploratory\n",
      "the agent chose the column that is  S\n",
      "The gain is incremented to 50.0\n",
      "The agent chose the smaller column, giving a reward of 11.0\n",
      "The current acumulated reward is 79.0\n",
      "Inmediate reward: 9.0\n",
      "update 8.928 =  8.928 + 0.8 *( 9.0 + 0.9 * 0.0 - 8.928 )\n",
      "The new value of ((1, 10.0, 'L', 'L'), 'S') is  8.9856\n",
      "END OF THE EPISODE _______________________________________________\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "State: depth: 0 , Bigger Column: 50 , First column to appear: L\n",
      "current_state setted to None\n",
      "Player looks for best value\n",
      "the agent chose the column that is  S\n",
      "The gain is incremented to 250\n",
      "The agent chose the smaller column, giving a reward of 70.0\n",
      "The current acumulated reward is 30.0\n",
      "Inmediate reward: 30.0\n",
      "update 277.223952384 =  277.223952384 + 0.8 *( 30.0 + 0.9 * 274.9824 - 277.223952384 )\n",
      "The new value of ((0, 50, 'L', 'R'), 'S') is  277.4321184768\n",
      "\n",
      "\n",
      "State: depth: 1 , Bigger Column: 250 , First column to appear: R\n",
      "current_state setted to ((0, 50, 'L', 'R'), 'S')\n",
      "Player looks for best value\n",
      "the agent chose the column that is  B\n",
      "the gain is reduced to 50.0\n",
      "The agent chose the bigger column, , giving a reward of 275.0\n",
      "The current acumulated reward is 305.0\n",
      "Inmediate reward: 275.0\n",
      "update 274.9824 =  274.9824 + 0.8 *( 275.0 + 0.9 * 0.0 - 274.9824 )\n",
      "The new value of ((1, 250, 'R', 'R'), 'B') is  274.99648\n",
      "END OF THE EPISODE _______________________________________________\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "State: depth: 0 , Bigger Column: 50 , First column to appear: R\n",
      "current_state setted to None\n",
      "Player looks for best value\n",
      "the agent chose the column that is  S\n",
      "The gain is incremented to 250\n",
      "The agent chose the smaller column, giving a reward of 70.0\n",
      "The current acumulated reward is 30.0\n",
      "Inmediate reward: 30.0\n",
      "update 241.10928101376 =  241.10928101376 + 0.8 *( 30.0 + 0.9 * 224.9856 - 241.10928101376 )\n",
      "The new value of ((0, 50, 'R', 'L'), 'S') is  234.211488202752\n",
      "\n",
      "\n",
      "State: depth: 1 , Bigger Column: 250 , First column to appear: L\n",
      "current_state setted to ((0, 50, 'R', 'L'), 'S')\n",
      "exploratory\n",
      "the agent chose the column that is  B\n",
      "the gain is reduced to 50.0\n",
      "The agent chose the bigger column, , giving a reward of 275.0\n",
      "The current acumulated reward is 305.0\n",
      "Inmediate reward: 275.0\n",
      "update 0.0 =  0.0 + 0.8 *( 275.0 + 0.9 * 0.0 - 0.0 )\n",
      "The new value of ((1, 250, 'L', 'R'), 'B') is  220.0\n",
      "END OF THE EPISODE _______________________________________________\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "State: depth: 0 , Bigger Column: 50 , First column to appear: L\n",
      "current_state setted to None\n",
      "Player looks for best value\n",
      "the agent chose the column that is  S\n",
      "The gain is incremented to 250\n",
      "The agent chose the smaller column, giving a reward of 70.0\n",
      "The current acumulated reward is 30.0\n",
      "Inmediate reward: 30.0\n",
      "update 277.4321184768 =  277.4321184768 + 0.8 *( 30.0 + 0.9 * 224.9856 - 277.4321184768 )\n",
      "The new value of ((0, 50, 'L', 'R'), 'S') is  241.47605569536\n",
      "\n",
      "\n",
      "State: depth: 1 , Bigger Column: 250 , First column to appear: L\n",
      "current_state setted to ((0, 50, 'L', 'R'), 'S')\n",
      "Player looks for best value\n",
      "the agent chose the column that is  S\n",
      "The gain is incremented to 1250\n",
      "The agent chose the smaller column, giving a reward of 275.0\n",
      "The current acumulated reward is 255.0\n",
      "Inmediate reward: 225.0\n",
      "update 224.9856 =  224.9856 + 0.8 *( 225.0 + 0.9 * 0.0 - 224.9856 )\n",
      "The new value of ((1, 250, 'L', 'R'), 'S') is  224.99712\n",
      "END OF THE EPISODE _______________________________________________\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "State: depth: 0 , Bigger Column: 50 , First column to appear: L\n",
      "current_state setted to None\n",
      "exploratory\n",
      "the agent chose the column that is  S\n",
      "The gain is incremented to 250\n",
      "The agent chose the smaller column, giving a reward of 70.0\n",
      "The current acumulated reward is 30.0\n",
      "Inmediate reward: 30.0\n",
      "update 241.47605569536 =  241.47605569536 + 0.8 *( 30.0 + 0.9 * 274.99648 - 241.47605569536 )\n",
      "The new value of ((0, 50, 'L', 'R'), 'S') is  270.292676739072\n",
      "\n",
      "\n",
      "State: depth: 1 , Bigger Column: 250 , First column to appear: R\n",
      "current_state setted to ((0, 50, 'L', 'R'), 'S')\n",
      "Player looks for best value\n",
      "the agent chose the column that is  B\n",
      "the gain is reduced to 50.0\n",
      "The agent chose the bigger column, , giving a reward of 275.0\n",
      "The current acumulated reward is 305.0\n",
      "Inmediate reward: 275.0\n",
      "update 274.99648 =  274.99648 + 0.8 *( 275.0 + 0.9 * 0.0 - 274.99648 )\n",
      "The new value of ((1, 250, 'R', 'L'), 'B') is  274.999296\n",
      "END OF THE EPISODE _______________________________________________\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "State: depth: 0 , Bigger Column: 50 , First column to appear: L\n",
      "current_state setted to None\n",
      "Player looks for best value\n",
      "the agent chose the column that is  S\n",
      "The gain is incremented to 250\n",
      "The agent chose the smaller column, giving a reward of 70.0\n",
      "The current acumulated reward is 30.0\n",
      "Inmediate reward: 30.0\n",
      "update 270.292676739072 =  270.292676739072 + 0.8 *( 30.0 + 0.9 * 274.999296 - 270.292676739072 )\n",
      "The new value of ((0, 50, 'L', 'R'), 'S') is  276.0580284678144\n",
      "\n",
      "\n",
      "State: depth: 1 , Bigger Column: 250 , First column to appear: R\n",
      "current_state setted to ((0, 50, 'L', 'R'), 'S')\n",
      "Player looks for best value\n",
      "the agent chose the column that is  B\n",
      "the gain is reduced to 50.0\n",
      "The agent chose the bigger column, , giving a reward of 275.0\n",
      "The current acumulated reward is 305.0\n",
      "Inmediate reward: 275.0\n",
      "update 274.999296 =  274.999296 + 0.8 *( 275.0 + 0.9 * 0.0 - 274.999296 )\n",
      "The new value of ((1, 250, 'R', 'L'), 'B') is  274.9998592\n",
      "END OF THE EPISODE _______________________________________________\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "State: depth: 0 , Bigger Column: 50 , First column to appear: R\n",
      "current_state setted to None\n",
      "Player looks for best value\n",
      "the agent chose the column that is  S\n",
      "The gain is incremented to 250\n",
      "The agent chose the smaller column, giving a reward of 70.0\n",
      "The current acumulated reward is 30.0\n",
      "Inmediate reward: 30.0\n",
      "update 234.211488202752 =  234.211488202752 + 0.8 *( 30.0 + 0.9 * 274.99648 - 234.211488202752 )\n",
      "The new value of ((0, 50, 'R', 'L'), 'S') is  268.83976324055044\n",
      "\n",
      "\n",
      "State: depth: 1 , Bigger Column: 250 , First column to appear: R\n",
      "current_state setted to ((0, 50, 'R', 'L'), 'S')\n",
      "Player looks for best value\n",
      "the agent chose the column that is  B\n",
      "the gain is reduced to 50.0\n",
      "The agent chose the bigger column, , giving a reward of 275.0\n",
      "The current acumulated reward is 305.0\n",
      "Inmediate reward: 275.0\n",
      "update 274.99648 =  274.99648 + 0.8 *( 275.0 + 0.9 * 0.0 - 274.99648 )\n",
      "The new value of ((1, 250, 'R', 'R'), 'B') is  274.999296\n",
      "END OF THE EPISODE _______________________________________________\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "State: depth: 0 , Bigger Column: 50 , First column to appear: R\n",
      "current_state setted to None\n",
      "Player looks for best value\n",
      "the agent chose the column that is  B\n",
      "the gain is reduced to 10.0\n",
      "The agent chose the bigger column, , giving a reward of 70.0\n",
      "The current acumulated reward is 70.0\n",
      "Inmediate reward: 70.0\n",
      "update 78.16955222605115 =  78.16955222605115 + 0.8 *( 70.0 + 0.9 * 10.56 - 78.16955222605115 )\n",
      "The new value of ((0, 50, 'R', 'R'), 'B') is  79.23711044521023\n",
      "\n",
      "\n",
      "State: depth: 1 , Bigger Column: 10.0 , First column to appear: R\n",
      "current_state setted to ((0, 50, 'R', 'R'), 'B')\n",
      "Player looks for best value\n",
      "the agent chose the column that is  B\n",
      "the gain is reduced to 2.0\n",
      "The agent chose the bigger column, , giving a reward of 11.0\n",
      "The current acumulated reward is 81.0\n",
      "Inmediate reward: 11.0\n",
      "update 10.56 =  10.56 + 0.8 *( 11.0 + 0.9 * 0.0 - 10.56 )\n",
      "The new value of ((1, 10.0, 'R', 'L'), 'B') is  10.912\n",
      "END OF THE EPISODE _______________________________________________\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "State: depth: 0 , Bigger Column: 50 , First column to appear: L\n",
      "current_state setted to None\n",
      "Player looks for best value\n",
      "the agent chose the column that is  S\n",
      "The gain is incremented to 250\n",
      "The agent chose the smaller column, giving a reward of 70.0\n",
      "The current acumulated reward is 30.0\n",
      "Inmediate reward: 30.0\n",
      "update 276.0580284678144 =  276.0580284678144 + 0.8 *( 30.0 + 0.9 * 274.9998592 - 276.0580284678144 )\n",
      "The new value of ((0, 50, 'L', 'R'), 'S') is  277.21150431756286\n",
      "\n",
      "\n",
      "State: depth: 1 , Bigger Column: 250 , First column to appear: R\n",
      "current_state setted to ((0, 50, 'L', 'R'), 'S')\n",
      "Player looks for best value\n",
      "the agent chose the column that is  B\n",
      "the gain is reduced to 50.0\n",
      "The agent chose the bigger column, , giving a reward of 275.0\n",
      "The current acumulated reward is 305.0\n",
      "Inmediate reward: 275.0\n",
      "update 274.9998592 =  274.9998592 + 0.8 *( 275.0 + 0.9 * 0.0 - 274.9998592 )\n",
      "The new value of ((1, 250, 'R', 'L'), 'B') is  274.99997184\n",
      "END OF THE EPISODE _______________________________________________\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "State: depth: 0 , Bigger Column: 50 , First column to appear: R\n",
      "current_state setted to None\n",
      "Player looks for best value\n",
      "the agent chose the column that is  S\n",
      "The gain is incremented to 250\n",
      "The agent chose the smaller column, giving a reward of 70.0\n",
      "The current acumulated reward is 30.0\n",
      "Inmediate reward: 30.0\n",
      "update 268.83976324055044 =  268.83976324055044 + 0.8 *( 30.0 + 0.9 * 274.56 - 268.83976324055044 )\n",
      "The new value of ((0, 50, 'R', 'L'), 'S') is  275.4511526481101\n",
      "\n",
      "\n",
      "State: depth: 1 , Bigger Column: 250 , First column to appear: L\n",
      "current_state setted to ((0, 50, 'R', 'L'), 'S')\n",
      "Player looks for best value\n",
      "the agent chose the column that is  B\n",
      "the gain is reduced to 50.0\n",
      "The agent chose the bigger column, , giving a reward of 275.0\n",
      "The current acumulated reward is 305.0\n",
      "Inmediate reward: 275.0\n",
      "update 274.56 =  274.56 + 0.8 *( 275.0 + 0.9 * 0.0 - 274.56 )\n",
      "The new value of ((1, 250, 'L', 'L'), 'B') is  274.912\n",
      "END OF THE EPISODE _______________________________________________\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "State: depth: 0 , Bigger Column: 50 , First column to appear: R\n",
      "current_state setted to None\n",
      "Player looks for best value\n",
      "the agent chose the column that is  S\n",
      "The gain is incremented to 250\n",
      "The agent chose the smaller column, giving a reward of 70.0\n",
      "The current acumulated reward is 30.0\n",
      "Inmediate reward: 30.0\n",
      "update 275.4511526481101 =  275.4511526481101 + 0.8 *( 30.0 + 0.9 * 274.99997184 - 275.4511526481101 )\n",
      "The new value of ((0, 50, 'R', 'L'), 'S') is  277.090210254422\n",
      "\n",
      "\n",
      "State: depth: 1 , Bigger Column: 250 , First column to appear: R\n",
      "current_state setted to ((0, 50, 'R', 'L'), 'S')\n",
      "exploratory\n",
      "the agent chose the column that is  B\n",
      "the gain is reduced to 50.0\n",
      "The agent chose the bigger column, , giving a reward of 275.0\n",
      "The current acumulated reward is 305.0\n",
      "Inmediate reward: 275.0\n",
      "update 274.99997184 =  274.99997184 + 0.8 *( 275.0 + 0.9 * 0.0 - 274.99997184 )\n",
      "The new value of ((1, 250, 'R', 'L'), 'B') is  274.999994368\n",
      "END OF THE EPISODE _______________________________________________\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "State: depth: 0 , Bigger Column: 50 , First column to appear: L\n",
      "current_state setted to None\n",
      "Player looks for best value\n",
      "the agent chose the column that is  S\n",
      "The gain is incremented to 250\n",
      "The agent chose the smaller column, giving a reward of 70.0\n",
      "The current acumulated reward is 30.0\n",
      "Inmediate reward: 30.0\n",
      "update 277.21150431756286 =  277.21150431756286 + 0.8 *( 30.0 + 0.9 * 274.999994368 - 277.21150431756286 )\n",
      "The new value of ((0, 50, 'L', 'R'), 'S') is  277.4422968084726\n",
      "\n",
      "\n",
      "State: depth: 1 , Bigger Column: 250 , First column to appear: R\n",
      "current_state setted to ((0, 50, 'L', 'R'), 'S')\n",
      "Player looks for best value\n",
      "the agent chose the column that is  B\n",
      "the gain is reduced to 50.0\n",
      "The agent chose the bigger column, , giving a reward of 275.0\n",
      "The current acumulated reward is 305.0\n",
      "Inmediate reward: 275.0\n",
      "update 274.999994368 =  274.999994368 + 0.8 *( 275.0 + 0.9 * 0.0 - 274.999994368 )\n",
      "The new value of ((1, 250, 'R', 'L'), 'B') is  274.9999988736\n",
      "END OF THE EPISODE _______________________________________________\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "State: depth: 0 , Bigger Column: 50 , First column to appear: R\n",
      "current_state setted to None\n",
      "Player looks for best value\n",
      "the agent chose the column that is  B\n",
      "the gain is reduced to 10.0\n",
      "The agent chose the bigger column, , giving a reward of 70.0\n",
      "The current acumulated reward is 70.0\n",
      "Inmediate reward: 70.0\n",
      "update 79.23711044521023 =  79.23711044521023 + 0.8 *( 70.0 + 0.9 * 10.912 - 79.23711044521023 )\n",
      "The new value of ((0, 50, 'R', 'R'), 'B') is  79.70406208904205\n",
      "\n",
      "\n",
      "State: depth: 1 , Bigger Column: 10.0 , First column to appear: R\n",
      "current_state setted to ((0, 50, 'R', 'R'), 'B')\n",
      "Player looks for best value\n",
      "the agent chose the column that is  B\n",
      "the gain is reduced to 2.0\n",
      "The agent chose the bigger column, , giving a reward of 11.0\n",
      "The current acumulated reward is 81.0\n",
      "Inmediate reward: 11.0\n",
      "update 10.912 =  10.912 + 0.8 *( 11.0 + 0.9 * 0.0 - 10.912 )\n",
      "The new value of ((1, 10.0, 'R', 'L'), 'B') is  10.9824\n",
      "END OF THE EPISODE _______________________________________________\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "State: depth: 0 , Bigger Column: 50 , First column to appear: R\n",
      "current_state setted to None\n",
      "Player looks for best value\n",
      "the agent chose the column that is  B\n",
      "the gain is reduced to 10.0\n",
      "The agent chose the bigger column, , giving a reward of 70.0\n",
      "The current acumulated reward is 70.0\n",
      "Inmediate reward: 70.0\n",
      "update 79.70406208904205 =  79.70406208904205 + 0.8 *( 70.0 + 0.9 * 10.9824 - 79.70406208904205 )\n",
      "The new value of ((0, 50, 'R', 'R'), 'B') is  79.84814041780841\n",
      "\n",
      "\n",
      "State: depth: 1 , Bigger Column: 10.0 , First column to appear: R\n",
      "current_state setted to ((0, 50, 'R', 'R'), 'B')\n",
      "Player looks for best value\n",
      "the agent chose the column that is  B\n",
      "the gain is reduced to 2.0\n",
      "The agent chose the bigger column, , giving a reward of 11.0\n",
      "The current acumulated reward is 81.0\n",
      "Inmediate reward: 11.0\n",
      "update 10.9824 =  10.9824 + 0.8 *( 11.0 + 0.9 * 0.0 - 10.9824 )\n",
      "The new value of ((1, 10.0, 'R', 'L'), 'B') is  10.99648\n",
      "END OF THE EPISODE _______________________________________________\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "State: depth: 0 , Bigger Column: 50 , First column to appear: L\n",
      "current_state setted to None\n",
      "Player looks for best value\n",
      "the agent chose the column that is  B\n",
      "the gain is reduced to 10.0\n",
      "The agent chose the bigger column, , giving a reward of 70.0\n",
      "The current acumulated reward is 70.0\n",
      "Inmediate reward: 70.0\n",
      "update 78.051623696964 =  78.051623696964 + 0.8 *( 70.0 + 0.9 * 10.99997184 - 78.051623696964 )\n",
      "The new value of ((0, 50, 'L', 'L'), 'B') is  79.5303044641928\n",
      "\n",
      "\n",
      "State: depth: 1 , Bigger Column: 10.0 , First column to appear: L\n",
      "current_state setted to ((0, 50, 'L', 'L'), 'B')\n",
      "Player looks for best value\n",
      "the agent chose the column that is  B\n",
      "the gain is reduced to 2.0\n",
      "The agent chose the bigger column, , giving a reward of 11.0\n",
      "The current acumulated reward is 81.0\n",
      "Inmediate reward: 11.0\n",
      "update 10.99997184 =  10.99997184 + 0.8 *( 11.0 + 0.9 * 0.0 - 10.99997184 )\n",
      "The new value of ((1, 10.0, 'L', 'R'), 'B') is  10.999994368\n",
      "END OF THE EPISODE _______________________________________________\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "State: depth: 0 , Bigger Column: 50 , First column to appear: R\n",
      "current_state setted to None\n",
      "Player looks for best value\n",
      "the agent chose the column that is  S\n",
      "The gain is incremented to 250\n",
      "The agent chose the smaller column, giving a reward of 70.0\n",
      "The current acumulated reward is 30.0\n",
      "Inmediate reward: 30.0\n",
      "update 277.090210254422 =  277.090210254422 + 0.8 *( 30.0 + 0.9 * 274.912 - 277.090210254422 )\n",
      "The new value of ((0, 50, 'R', 'L'), 'S') is  277.3546820508844\n",
      "\n",
      "\n",
      "State: depth: 1 , Bigger Column: 250 , First column to appear: L\n",
      "current_state setted to ((0, 50, 'R', 'L'), 'S')\n",
      "Player looks for best value\n",
      "the agent chose the column that is  B\n",
      "the gain is reduced to 50.0\n",
      "The agent chose the bigger column, , giving a reward of 275.0\n",
      "The current acumulated reward is 305.0\n",
      "Inmediate reward: 275.0\n",
      "update 274.912 =  274.912 + 0.8 *( 275.0 + 0.9 * 0.0 - 274.912 )\n",
      "The new value of ((1, 250, 'L', 'L'), 'B') is  274.9824\n",
      "END OF THE EPISODE _______________________________________________\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "State: depth: 0 , Bigger Column: 50 , First column to appear: L\n",
      "current_state setted to None\n",
      "Player looks for best value\n",
      "the agent chose the column that is  S\n",
      "The gain is incremented to 250\n",
      "The agent chose the smaller column, giving a reward of 70.0\n",
      "The current acumulated reward is 30.0\n",
      "Inmediate reward: 30.0\n",
      "update 277.4422968084726 =  277.4422968084726 + 0.8 *( 30.0 + 0.9 * 274.9999988736 - 277.4422968084726 )\n",
      "The new value of ((0, 50, 'L', 'R'), 'S') is  277.4884585506865\n",
      "\n",
      "\n",
      "State: depth: 1 , Bigger Column: 250 , First column to appear: R\n",
      "current_state setted to ((0, 50, 'L', 'R'), 'S')\n",
      "Player looks for best value\n",
      "the agent chose the column that is  B\n",
      "the gain is reduced to 50.0\n",
      "The agent chose the bigger column, , giving a reward of 275.0\n",
      "The current acumulated reward is 305.0\n",
      "Inmediate reward: 275.0\n",
      "update 274.9999988736 =  274.9999988736 + 0.8 *( 275.0 + 0.9 * 0.0 - 274.9999988736 )\n",
      "The new value of ((1, 250, 'R', 'L'), 'B') is  274.99999977472\n",
      "END OF THE EPISODE _______________________________________________\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "State: depth: 0 , Bigger Column: 50 , First column to appear: R\n",
      "current_state setted to None\n",
      "Player looks for best value\n",
      "the agent chose the column that is  B\n",
      "the gain is reduced to 10.0\n",
      "The agent chose the bigger column, , giving a reward of 70.0\n",
      "The current acumulated reward is 70.0\n",
      "Inmediate reward: 70.0\n",
      "update 79.84814041780841 =  79.84814041780841 + 0.8 *( 70.0 + 0.9 * 10.99648 - 79.84814041780841 )\n",
      "The new value of ((0, 50, 'R', 'R'), 'B') is  79.88709368356169\n",
      "\n",
      "\n",
      "State: depth: 1 , Bigger Column: 10.0 , First column to appear: R\n",
      "current_state setted to ((0, 50, 'R', 'R'), 'B')\n",
      "Player looks for best value\n",
      "the agent chose the column that is  B\n",
      "the gain is reduced to 2.0\n",
      "The agent chose the bigger column, , giving a reward of 11.0\n",
      "The current acumulated reward is 81.0\n",
      "Inmediate reward: 11.0\n",
      "update 10.99648 =  10.99648 + 0.8 *( 11.0 + 0.9 * 0.0 - 10.99648 )\n",
      "The new value of ((1, 10.0, 'R', 'L'), 'B') is  10.999296\n",
      "END OF THE EPISODE _______________________________________________\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "State: depth: 0 , Bigger Column: 50 , First column to appear: L\n",
      "current_state setted to None\n",
      "Player looks for best value\n",
      "the agent chose the column that is  S\n",
      "The gain is incremented to 250\n",
      "The agent chose the smaller column, giving a reward of 70.0\n",
      "The current acumulated reward is 30.0\n",
      "Inmediate reward: 30.0\n",
      "update 277.4884585506865 =  277.4884585506865 + 0.8 *( 30.0 + 0.9 * 274.99999977472 - 277.4884585506865 )\n",
      "The new value of ((0, 50, 'L', 'R'), 'S') is  277.4976915479357\n",
      "\n",
      "\n",
      "State: depth: 1 , Bigger Column: 250 , First column to appear: R\n",
      "current_state setted to ((0, 50, 'L', 'R'), 'S')\n",
      "Player looks for best value\n",
      "the agent chose the column that is  B\n",
      "the gain is reduced to 50.0\n",
      "The agent chose the bigger column, , giving a reward of 275.0\n",
      "The current acumulated reward is 305.0\n",
      "Inmediate reward: 275.0\n",
      "update 274.99999977472 =  274.99999977472 + 0.8 *( 275.0 + 0.9 * 0.0 - 274.99999977472 )\n",
      "The new value of ((1, 250, 'R', 'L'), 'B') is  274.999999954944\n",
      "END OF THE EPISODE _______________________________________________\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "State: depth: 0 , Bigger Column: 50 , First column to appear: L\n",
      "current_state setted to None\n",
      "Player looks for best value\n",
      "the agent chose the column that is  B\n",
      "the gain is reduced to 10.0\n",
      "The agent chose the bigger column, , giving a reward of 70.0\n",
      "The current acumulated reward is 70.0\n",
      "Inmediate reward: 70.0\n",
      "update 79.5303044641928 =  79.5303044641928 + 0.8 *( 70.0 + 0.9 * 8.9856 - 79.5303044641928 )\n",
      "The new value of ((0, 50, 'L', 'L'), 'B') is  78.37569289283856\n",
      "\n",
      "\n",
      "State: depth: 1 , Bigger Column: 10.0 , First column to appear: L\n",
      "current_state setted to ((0, 50, 'L', 'L'), 'B')\n",
      "Player looks for best value\n",
      "the agent chose the column that is  S\n",
      "The gain is incremented to 50.0\n",
      "The agent chose the smaller column, giving a reward of 11.0\n",
      "The current acumulated reward is 79.0\n",
      "Inmediate reward: 9.0\n",
      "update 8.9856 =  8.9856 + 0.8 *( 9.0 + 0.9 * 0.0 - 8.9856 )\n",
      "The new value of ((1, 10.0, 'L', 'L'), 'S') is  8.99712\n",
      "END OF THE EPISODE _______________________________________________\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "State: depth: 0 , Bigger Column: 50 , First column to appear: L\n",
      "current_state setted to None\n",
      "exploratory\n",
      "the agent chose the column that is  S\n",
      "The gain is incremented to 250\n",
      "The agent chose the smaller column, giving a reward of 70.0\n",
      "The current acumulated reward is 30.0\n",
      "Inmediate reward: 30.0\n",
      "update 0.0 =  0.0 + 0.8 *( 30.0 + 0.9 * 224.99712 - 0.0 )\n",
      "The new value of ((0, 50, 'L', 'L'), 'S') is  185.9979264\n",
      "\n",
      "\n",
      "State: depth: 1 , Bigger Column: 250 , First column to appear: L\n",
      "current_state setted to ((0, 50, 'L', 'L'), 'S')\n",
      "Player looks for best value\n",
      "the agent chose the column that is  S\n",
      "The gain is incremented to 1250\n",
      "The agent chose the smaller column, giving a reward of 275.0\n",
      "The current acumulated reward is 255.0\n",
      "Inmediate reward: 225.0\n",
      "update 224.99712 =  224.99712 + 0.8 *( 225.0 + 0.9 * 0.0 - 224.99712 )\n",
      "The new value of ((1, 250, 'L', 'R'), 'S') is  224.999424\n",
      "END OF THE EPISODE _______________________________________________\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "State: depth: 0 , Bigger Column: 50 , First column to appear: R\n",
      "current_state setted to None\n",
      "Player looks for best value\n",
      "the agent chose the column that is  S\n",
      "The gain is incremented to 250\n",
      "The agent chose the smaller column, giving a reward of 70.0\n",
      "The current acumulated reward is 30.0\n",
      "Inmediate reward: 30.0\n",
      "update 277.3546820508844 =  277.3546820508844 + 0.8 *( 30.0 + 0.9 * 274.999296 - 277.3546820508844 )\n",
      "The new value of ((0, 50, 'R', 'L'), 'S') is  277.4704295301769\n",
      "\n",
      "\n",
      "State: depth: 1 , Bigger Column: 250 , First column to appear: R\n",
      "current_state setted to ((0, 50, 'R', 'L'), 'S')\n",
      "Player looks for best value\n",
      "the agent chose the column that is  B\n",
      "the gain is reduced to 50.0\n",
      "The agent chose the bigger column, , giving a reward of 275.0\n",
      "The current acumulated reward is 305.0\n",
      "Inmediate reward: 275.0\n",
      "update 274.999296 =  274.999296 + 0.8 *( 275.0 + 0.9 * 0.0 - 274.999296 )\n",
      "The new value of ((1, 250, 'R', 'R'), 'B') is  274.9998592\n",
      "END OF THE EPISODE _______________________________________________\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "State: depth: 0 , Bigger Column: 50 , First column to appear: R\n",
      "current_state setted to None\n",
      "Player looks for best value\n",
      "the agent chose the column that is  S\n",
      "The gain is incremented to 250\n",
      "The agent chose the smaller column, giving a reward of 70.0\n",
      "The current acumulated reward is 30.0\n",
      "Inmediate reward: 30.0\n",
      "update 277.4704295301769 =  277.4704295301769 + 0.8 *( 30.0 + 0.9 * 274.999999954944 - 277.4704295301769 )\n",
      "The new value of ((0, 50, 'R', 'L'), 'S') is  277.49408587359505\n",
      "\n",
      "\n",
      "State: depth: 1 , Bigger Column: 250 , First column to appear: R\n",
      "current_state setted to ((0, 50, 'R', 'L'), 'S')\n",
      "Player looks for best value\n",
      "the agent chose the column that is  B\n",
      "the gain is reduced to 50.0\n",
      "The agent chose the bigger column, , giving a reward of 275.0\n",
      "The current acumulated reward is 305.0\n",
      "Inmediate reward: 275.0\n",
      "update 274.999999954944 =  274.999999954944 + 0.8 *( 275.0 + 0.9 * 0.0 - 274.999999954944 )\n",
      "The new value of ((1, 250, 'R', 'L'), 'B') is  274.9999999909888\n",
      "END OF THE EPISODE _______________________________________________\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "State: depth: 0 , Bigger Column: 50 , First column to appear: R\n",
      "current_state setted to None\n",
      "Player looks for best value\n",
      "the agent chose the column that is  S\n",
      "The gain is incremented to 250\n",
      "The agent chose the smaller column, giving a reward of 70.0\n",
      "The current acumulated reward is 30.0\n",
      "Inmediate reward: 30.0\n",
      "update 277.49408587359505 =  277.49408587359505 + 0.8 *( 30.0 + 0.9 * 224.999424 - 277.49408587359505 )\n",
      "The new value of ((0, 50, 'R', 'L'), 'S') is  241.498402454719\n",
      "\n",
      "\n",
      "State: depth: 1 , Bigger Column: 250 , First column to appear: L\n",
      "current_state setted to ((0, 50, 'R', 'L'), 'S')\n",
      "Player looks for best value\n",
      "the agent chose the column that is  S\n",
      "The gain is incremented to 1250\n",
      "The agent chose the smaller column, giving a reward of 275.0\n",
      "The current acumulated reward is 255.0\n",
      "Inmediate reward: 225.0\n",
      "update 224.999424 =  224.999424 + 0.8 *( 225.0 + 0.9 * 0.0 - 224.999424 )\n",
      "The new value of ((1, 250, 'L', 'R'), 'S') is  224.9998848\n",
      "END OF THE EPISODE _______________________________________________\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "State: depth: 0 , Bigger Column: 50 , First column to appear: R\n",
      "current_state setted to None\n",
      "exploratory\n",
      "the agent chose the column that is  S\n",
      "The gain is incremented to 250\n",
      "The agent chose the smaller column, giving a reward of 70.0\n",
      "The current acumulated reward is 30.0\n",
      "Inmediate reward: 30.0\n",
      "update 241.498402454719 =  241.498402454719 + 0.8 *( 30.0 + 0.9 * 224.9998848 - 241.498402454719 )\n",
      "The new value of ((0, 50, 'R', 'L'), 'S') is  234.2995975469438\n",
      "\n",
      "\n",
      "State: depth: 1 , Bigger Column: 250 , First column to appear: L\n",
      "current_state setted to ((0, 50, 'R', 'L'), 'S')\n",
      "Player looks for best value\n",
      "the agent chose the column that is  S\n",
      "The gain is incremented to 1250\n",
      "The agent chose the smaller column, giving a reward of 275.0\n",
      "The current acumulated reward is 255.0\n",
      "Inmediate reward: 225.0\n",
      "update 224.9998848 =  224.9998848 + 0.8 *( 225.0 + 0.9 * 0.0 - 224.9998848 )\n",
      "The new value of ((1, 250, 'L', 'R'), 'S') is  224.99997696\n",
      "END OF THE EPISODE _______________________________________________\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "State: depth: 0 , Bigger Column: 50 , First column to appear: L\n",
      "current_state setted to None\n",
      "Player looks for best value\n",
      "the agent chose the column that is  S\n",
      "The gain is incremented to 250\n",
      "The agent chose the smaller column, giving a reward of 70.0\n",
      "The current acumulated reward is 30.0\n",
      "Inmediate reward: 30.0\n",
      "update 185.9979264 =  185.9979264 + 0.8 *( 30.0 + 0.9 * 224.99997696 - 185.9979264 )\n",
      "The new value of ((0, 50, 'L', 'L'), 'S') is  223.1995686912\n",
      "\n",
      "\n",
      "State: depth: 1 , Bigger Column: 250 , First column to appear: L\n",
      "current_state setted to ((0, 50, 'L', 'L'), 'S')\n",
      "Player looks for best value\n",
      "the agent chose the column that is  S\n",
      "The gain is incremented to 1250\n",
      "The agent chose the smaller column, giving a reward of 275.0\n",
      "The current acumulated reward is 255.0\n",
      "Inmediate reward: 225.0\n",
      "update 224.99997696 =  224.99997696 + 0.8 *( 225.0 + 0.9 * 0.0 - 224.99997696 )\n",
      "The new value of ((1, 250, 'L', 'R'), 'S') is  224.999995392\n",
      "END OF THE EPISODE _______________________________________________\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "State: depth: 0 , Bigger Column: 50 , First column to appear: L\n",
      "current_state setted to None\n",
      "Player looks for best value\n",
      "the agent chose the column that is  S\n",
      "The gain is incremented to 250\n",
      "The agent chose the smaller column, giving a reward of 70.0\n",
      "The current acumulated reward is 30.0\n",
      "Inmediate reward: 30.0\n",
      "update 277.4976915479357 =  277.4976915479357 + 0.8 *( 30.0 + 0.9 * 274.9999999909888 - 277.4976915479357 )\n",
      "The new value of ((0, 50, 'L', 'R'), 'S') is  277.4995383030991\n",
      "\n",
      "\n",
      "State: depth: 1 , Bigger Column: 250 , First column to appear: R\n",
      "current_state setted to ((0, 50, 'L', 'R'), 'S')\n",
      "Player looks for best value\n",
      "the agent chose the column that is  B\n",
      "the gain is reduced to 50.0\n",
      "The agent chose the bigger column, , giving a reward of 275.0\n",
      "The current acumulated reward is 305.0\n",
      "Inmediate reward: 275.0\n",
      "update 274.9999999909888 =  274.9999999909888 + 0.8 *( 275.0 + 0.9 * 0.0 - 274.9999999909888 )\n",
      "The new value of ((1, 250, 'R', 'L'), 'B') is  274.9999999981978\n",
      "END OF THE EPISODE _______________________________________________\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "State: depth: 0 , Bigger Column: 50 , First column to appear: R\n",
      "current_state setted to None\n",
      "Player looks for best value\n",
      "the agent chose the column that is  B\n",
      "the gain is reduced to 10.0\n",
      "The agent chose the bigger column, , giving a reward of 70.0\n",
      "The current acumulated reward is 70.0\n",
      "Inmediate reward: 70.0\n",
      "update 79.88709368356169 =  79.88709368356169 + 0.8 *( 70.0 + 0.9 * 10.999296 - 79.88709368356169 )\n",
      "The new value of ((0, 50, 'R', 'R'), 'B') is  79.89691185671234\n",
      "\n",
      "\n",
      "State: depth: 1 , Bigger Column: 10.0 , First column to appear: R\n",
      "current_state setted to ((0, 50, 'R', 'R'), 'B')\n",
      "Player looks for best value\n",
      "the agent chose the column that is  B\n",
      "the gain is reduced to 2.0\n",
      "The agent chose the bigger column, , giving a reward of 11.0\n",
      "The current acumulated reward is 81.0\n",
      "Inmediate reward: 11.0\n",
      "update 10.999296 =  10.999296 + 0.8 *( 11.0 + 0.9 * 0.0 - 10.999296 )\n",
      "The new value of ((1, 10.0, 'R', 'L'), 'B') is  10.9998592\n",
      "END OF THE EPISODE _______________________________________________\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "State: depth: 0 , Bigger Column: 50 , First column to appear: R\n",
      "current_state setted to None\n",
      "Player looks for best value\n",
      "the agent chose the column that is  S\n",
      "The gain is incremented to 250\n",
      "The agent chose the smaller column, giving a reward of 70.0\n",
      "The current acumulated reward is 30.0\n",
      "Inmediate reward: 30.0\n",
      "update 234.2995975469438 =  234.2995975469438 + 0.8 *( 30.0 + 0.9 * 224.999995392 - 234.2995975469438 )\n",
      "The new value of ((0, 50, 'R', 'L'), 'S') is  232.85991619162877\n",
      "\n",
      "\n",
      "State: depth: 1 , Bigger Column: 250 , First column to appear: L\n",
      "current_state setted to ((0, 50, 'R', 'L'), 'S')\n",
      "Player looks for best value\n",
      "the agent chose the column that is  S\n",
      "The gain is incremented to 1250\n",
      "The agent chose the smaller column, giving a reward of 275.0\n",
      "The current acumulated reward is 255.0\n",
      "Inmediate reward: 225.0\n",
      "update 224.999995392 =  224.999995392 + 0.8 *( 225.0 + 0.9 * 0.0 - 224.999995392 )\n",
      "The new value of ((1, 250, 'L', 'R'), 'S') is  224.9999990784\n",
      "END OF THE EPISODE _______________________________________________\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "State: depth: 0 , Bigger Column: 50 , First column to appear: L\n",
      "current_state setted to None\n",
      "Player looks for best value\n",
      "the agent chose the column that is  S\n",
      "The gain is incremented to 250\n",
      "The agent chose the smaller column, giving a reward of 70.0\n",
      "The current acumulated reward is 30.0\n",
      "Inmediate reward: 30.0\n",
      "update 223.1995686912 =  223.1995686912 + 0.8 *( 30.0 + 0.9 * 274.9999999981978 - 223.1995686912 )\n",
      "The new value of ((0, 50, 'L', 'L'), 'S') is  266.63991373694245\n",
      "\n",
      "\n",
      "State: depth: 1 , Bigger Column: 250 , First column to appear: R\n",
      "current_state setted to ((0, 50, 'L', 'L'), 'S')\n",
      "Player looks for best value\n",
      "the agent chose the column that is  B\n",
      "the gain is reduced to 50.0\n",
      "The agent chose the bigger column, , giving a reward of 275.0\n",
      "The current acumulated reward is 305.0\n",
      "Inmediate reward: 275.0\n",
      "update 274.9999999981978 =  274.9999999981978 + 0.8 *( 275.0 + 0.9 * 0.0 - 274.9999999981978 )\n",
      "The new value of ((1, 250, 'R', 'L'), 'B') is  274.99999999963956\n",
      "END OF THE EPISODE _______________________________________________\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "State: depth: 0 , Bigger Column: 50 , First column to appear: L\n",
      "current_state setted to None\n",
      "Player looks for best value\n",
      "the agent chose the column that is  S\n",
      "The gain is incremented to 250\n",
      "The agent chose the smaller column, giving a reward of 70.0\n",
      "The current acumulated reward is 30.0\n",
      "Inmediate reward: 30.0\n",
      "update 277.4995383030991 =  277.4995383030991 + 0.8 *( 30.0 + 0.9 * 224.9999990784 - 277.4995383030991 )\n",
      "The new value of ((0, 50, 'L', 'R'), 'S') is  241.49990699706782\n",
      "\n",
      "\n",
      "State: depth: 1 , Bigger Column: 250 , First column to appear: L\n",
      "current_state setted to ((0, 50, 'L', 'R'), 'S')\n",
      "Player looks for best value\n",
      "the agent chose the column that is  S\n",
      "The gain is incremented to 1250\n",
      "The agent chose the smaller column, giving a reward of 275.0\n",
      "The current acumulated reward is 255.0\n",
      "Inmediate reward: 225.0\n",
      "update 224.9999990784 =  224.9999990784 + 0.8 *( 225.0 + 0.9 * 0.0 - 224.9999990784 )\n",
      "The new value of ((1, 250, 'L', 'R'), 'S') is  224.99999981568\n",
      "END OF THE EPISODE _______________________________________________\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "State: depth: 0 , Bigger Column: 50 , First column to appear: L\n",
      "current_state setted to None\n",
      "Player looks for best value\n",
      "the agent chose the column that is  S\n",
      "The gain is incremented to 250\n",
      "The agent chose the smaller column, giving a reward of 70.0\n",
      "The current acumulated reward is 30.0\n",
      "Inmediate reward: 30.0\n",
      "update 266.63991373694245 =  266.63991373694245 + 0.8 *( 30.0 + 0.9 * 274.9824 - 266.63991373694245 )\n",
      "The new value of ((0, 50, 'L', 'L'), 'S') is  275.3153107473885\n",
      "\n",
      "\n",
      "State: depth: 1 , Bigger Column: 250 , First column to appear: L\n",
      "current_state setted to ((0, 50, 'L', 'L'), 'S')\n",
      "Player looks for best value\n",
      "the agent chose the column that is  B\n",
      "the gain is reduced to 50.0\n",
      "The agent chose the bigger column, , giving a reward of 275.0\n",
      "The current acumulated reward is 305.0\n",
      "Inmediate reward: 275.0\n",
      "update 274.9824 =  274.9824 + 0.8 *( 275.0 + 0.9 * 0.0 - 274.9824 )\n",
      "The new value of ((1, 250, 'L', 'L'), 'B') is  274.99648\n",
      "END OF THE EPISODE _______________________________________________\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "State: depth: 0 , Bigger Column: 50 , First column to appear: L\n",
      "current_state setted to None\n",
      "Player looks for best value\n",
      "the agent chose the column that is  S\n",
      "The gain is incremented to 250\n",
      "The agent chose the smaller column, giving a reward of 70.0\n",
      "The current acumulated reward is 30.0\n",
      "Inmediate reward: 30.0\n",
      "update 241.49990699706782 =  241.49990699706782 + 0.8 *( 30.0 + 0.9 * 224.99999981568 - 241.49990699706782 )\n",
      "The new value of ((0, 50, 'L', 'R'), 'S') is  234.29998126670316\n",
      "\n",
      "\n",
      "State: depth: 1 , Bigger Column: 250 , First column to appear: L\n",
      "current_state setted to ((0, 50, 'L', 'R'), 'S')\n",
      "Player looks for best value\n",
      "the agent chose the column that is  S\n",
      "The gain is incremented to 1250\n",
      "The agent chose the smaller column, giving a reward of 275.0\n",
      "The current acumulated reward is 255.0\n",
      "Inmediate reward: 225.0\n",
      "update 224.99999981568 =  224.99999981568 + 0.8 *( 225.0 + 0.9 * 0.0 - 224.99999981568 )\n",
      "The new value of ((1, 250, 'L', 'R'), 'S') is  224.999999963136\n",
      "END OF THE EPISODE _______________________________________________\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "State: depth: 0 , Bigger Column: 50 , First column to appear: L\n",
      "current_state setted to None\n",
      "Player looks for best value\n",
      "the agent chose the column that is  S\n",
      "The gain is incremented to 250\n",
      "The agent chose the smaller column, giving a reward of 70.0\n",
      "The current acumulated reward is 30.0\n",
      "Inmediate reward: 30.0\n",
      "update 234.29998126670316 =  234.29998126670316 + 0.8 *( 30.0 + 0.9 * 274.99999999963956 - 234.29998126670316 )\n",
      "The new value of ((0, 50, 'L', 'R'), 'S') is  268.85999625308114\n",
      "\n",
      "\n",
      "State: depth: 1 , Bigger Column: 250 , First column to appear: R\n",
      "current_state setted to ((0, 50, 'L', 'R'), 'S')\n",
      "Player looks for best value\n",
      "the agent chose the column that is  B\n",
      "the gain is reduced to 50.0\n",
      "The agent chose the bigger column, , giving a reward of 275.0\n",
      "The current acumulated reward is 305.0\n",
      "Inmediate reward: 275.0\n",
      "update 274.99999999963956 =  274.99999999963956 + 0.8 *( 275.0 + 0.9 * 0.0 - 274.99999999963956 )\n",
      "The new value of ((1, 250, 'R', 'L'), 'B') is  274.9999999999279\n",
      "END OF THE EPISODE _______________________________________________\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "State: depth: 0 , Bigger Column: 50 , First column to appear: R\n",
      "current_state setted to None\n",
      "Player looks for best value\n",
      "the agent chose the column that is  S\n",
      "The gain is incremented to 250\n",
      "The agent chose the smaller column, giving a reward of 70.0\n",
      "The current acumulated reward is 30.0\n",
      "Inmediate reward: 30.0\n",
      "update 232.85991619162877 =  232.85991619162877 + 0.8 *( 30.0 + 0.9 * 274.9999999999279 - 232.85991619162877 )\n",
      "The new value of ((0, 50, 'R', 'L'), 'S') is  268.57198323827384\n",
      "\n",
      "\n",
      "State: depth: 1 , Bigger Column: 250 , First column to appear: R\n",
      "current_state setted to ((0, 50, 'R', 'L'), 'S')\n",
      "Player looks for best value\n",
      "the agent chose the column that is  B\n",
      "the gain is reduced to 50.0\n",
      "The agent chose the bigger column, , giving a reward of 275.0\n",
      "The current acumulated reward is 305.0\n",
      "Inmediate reward: 275.0\n",
      "update 274.9999999999279 =  274.9999999999279 + 0.8 *( 275.0 + 0.9 * 0.0 - 274.9999999999279 )\n",
      "The new value of ((1, 250, 'R', 'L'), 'B') is  274.99999999998556\n",
      "END OF THE EPISODE _______________________________________________\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "State: depth: 0 , Bigger Column: 50 , First column to appear: L\n",
      "current_state setted to None\n",
      "Player looks for best value\n",
      "the agent chose the column that is  S\n",
      "The gain is incremented to 250\n",
      "The agent chose the smaller column, giving a reward of 70.0\n",
      "The current acumulated reward is 30.0\n",
      "Inmediate reward: 30.0\n",
      "update 275.3153107473885 =  275.3153107473885 + 0.8 *( 30.0 + 0.9 * 274.99999999998556 - 275.3153107473885 )\n",
      "The new value of ((0, 50, 'L', 'L'), 'S') is  277.0630621494673\n",
      "\n",
      "\n",
      "State: depth: 1 , Bigger Column: 250 , First column to appear: R\n",
      "current_state setted to ((0, 50, 'L', 'L'), 'S')\n",
      "Player looks for best value\n",
      "the agent chose the column that is  B\n",
      "the gain is reduced to 50.0\n",
      "The agent chose the bigger column, , giving a reward of 275.0\n",
      "The current acumulated reward is 305.0\n",
      "Inmediate reward: 275.0\n",
      "update 274.99999999998556 =  274.99999999998556 + 0.8 *( 275.0 + 0.9 * 0.0 - 274.99999999998556 )\n",
      "The new value of ((1, 250, 'R', 'L'), 'B') is  274.9999999999971\n",
      "END OF THE EPISODE _______________________________________________\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "State: depth: 0 , Bigger Column: 50 , First column to appear: R\n",
      "current_state setted to None\n",
      "Player looks for best value\n",
      "the agent chose the column that is  S\n",
      "The gain is incremented to 250\n",
      "The agent chose the smaller column, giving a reward of 70.0\n",
      "The current acumulated reward is 30.0\n",
      "Inmediate reward: 30.0\n",
      "update 268.57198323827384 =  268.57198323827384 + 0.8 *( 30.0 + 0.9 * 274.9999999999971 - 268.57198323827384 )\n",
      "The new value of ((0, 50, 'R', 'L'), 'S') is  275.7143966476527\n",
      "\n",
      "\n",
      "State: depth: 1 , Bigger Column: 250 , First column to appear: R\n",
      "current_state setted to ((0, 50, 'R', 'L'), 'S')\n",
      "Player looks for best value\n",
      "the agent chose the column that is  B\n",
      "the gain is reduced to 50.0\n",
      "The agent chose the bigger column, , giving a reward of 275.0\n",
      "The current acumulated reward is 305.0\n",
      "Inmediate reward: 275.0\n",
      "update 274.9999999999971 =  274.9999999999971 + 0.8 *( 275.0 + 0.9 * 0.0 - 274.9999999999971 )\n",
      "The new value of ((1, 250, 'R', 'L'), 'B') is  274.99999999999943\n",
      "END OF THE EPISODE _______________________________________________\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "State: depth: 0 , Bigger Column: 50 , First column to appear: R\n",
      "current_state setted to None\n",
      "Player looks for best value\n",
      "the agent chose the column that is  S\n",
      "The gain is incremented to 250\n",
      "The agent chose the smaller column, giving a reward of 70.0\n",
      "The current acumulated reward is 30.0\n",
      "Inmediate reward: 30.0\n",
      "update 275.7143966476527 =  275.7143966476527 + 0.8 *( 30.0 + 0.9 * 274.99999999999943 - 275.7143966476527 )\n",
      "The new value of ((0, 50, 'R', 'L'), 'S') is  277.1428793295301\n",
      "\n",
      "\n",
      "State: depth: 1 , Bigger Column: 250 , First column to appear: R\n",
      "current_state setted to ((0, 50, 'R', 'L'), 'S')\n",
      "Player looks for best value\n",
      "the agent chose the column that is  B\n",
      "the gain is reduced to 50.0\n",
      "The agent chose the bigger column, , giving a reward of 275.0\n",
      "The current acumulated reward is 305.0\n",
      "Inmediate reward: 275.0\n",
      "update 274.99999999999943 =  274.99999999999943 + 0.8 *( 275.0 + 0.9 * 0.0 - 274.99999999999943 )\n",
      "The new value of ((1, 250, 'R', 'L'), 'B') is  274.9999999999999\n",
      "END OF THE EPISODE _______________________________________________\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "State: depth: 0 , Bigger Column: 50 , First column to appear: R\n",
      "current_state setted to None\n",
      "Player looks for best value\n",
      "the agent chose the column that is  S\n",
      "The gain is incremented to 250\n",
      "The agent chose the smaller column, giving a reward of 70.0\n",
      "The current acumulated reward is 30.0\n",
      "Inmediate reward: 30.0\n",
      "update 277.1428793295301 =  277.1428793295301 + 0.8 *( 30.0 + 0.9 * 274.9998592 - 277.1428793295301 )\n",
      "The new value of ((0, 50, 'R', 'L'), 'S') is  277.428474489906\n",
      "\n",
      "\n",
      "State: depth: 1 , Bigger Column: 250 , First column to appear: R\n",
      "current_state setted to ((0, 50, 'R', 'L'), 'S')\n",
      "Player looks for best value\n",
      "the agent chose the column that is  B\n",
      "the gain is reduced to 50.0\n",
      "The agent chose the bigger column, , giving a reward of 275.0\n",
      "The current acumulated reward is 305.0\n",
      "Inmediate reward: 275.0\n",
      "update 274.9998592 =  274.9998592 + 0.8 *( 275.0 + 0.9 * 0.0 - 274.9998592 )\n",
      "The new value of ((1, 250, 'R', 'R'), 'B') is  274.99997184\n",
      "END OF THE EPISODE _______________________________________________\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "State: depth: 0 , Bigger Column: 50 , First column to appear: R\n",
      "current_state setted to None\n",
      "Player looks for best value\n",
      "the agent chose the column that is  S\n",
      "The gain is incremented to 250\n",
      "The agent chose the smaller column, giving a reward of 70.0\n",
      "The current acumulated reward is 30.0\n",
      "Inmediate reward: 30.0\n",
      "update 277.428474489906 =  277.428474489906 + 0.8 *( 30.0 + 0.9 * 224.999999963136 - 277.428474489906 )\n",
      "The new value of ((0, 50, 'R', 'L'), 'S') is  241.48569487143914\n",
      "\n",
      "\n",
      "State: depth: 1 , Bigger Column: 250 , First column to appear: L\n",
      "current_state setted to ((0, 50, 'R', 'L'), 'S')\n",
      "Player looks for best value\n",
      "the agent chose the column that is  S\n",
      "The gain is incremented to 1250\n",
      "The agent chose the smaller column, giving a reward of 275.0\n",
      "The current acumulated reward is 255.0\n",
      "Inmediate reward: 225.0\n",
      "update 224.999999963136 =  224.999999963136 + 0.8 *( 225.0 + 0.9 * 0.0 - 224.999999963136 )\n",
      "The new value of ((1, 250, 'L', 'R'), 'S') is  224.9999999926272\n",
      "END OF THE EPISODE _______________________________________________\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "State: depth: 0 , Bigger Column: 50 , First column to appear: L\n",
      "current_state setted to None\n",
      "Player looks for best value\n",
      "the agent chose the column that is  S\n",
      "The gain is incremented to 250\n",
      "The agent chose the smaller column, giving a reward of 70.0\n",
      "The current acumulated reward is 30.0\n",
      "Inmediate reward: 30.0\n",
      "update 268.85999625308114 =  268.85999625308114 + 0.8 *( 30.0 + 0.9 * 274.9999999999999 - 268.85999625308114 )\n",
      "The new value of ((0, 50, 'L', 'R'), 'S') is  275.77199925061615\n",
      "\n",
      "\n",
      "State: depth: 1 , Bigger Column: 250 , First column to appear: R\n",
      "current_state setted to ((0, 50, 'L', 'R'), 'S')\n",
      "Player looks for best value\n",
      "the agent chose the column that is  B\n",
      "the gain is reduced to 50.0\n",
      "The agent chose the bigger column, , giving a reward of 275.0\n",
      "The current acumulated reward is 305.0\n",
      "Inmediate reward: 275.0\n",
      "update 274.9999999999999 =  274.9999999999999 + 0.8 *( 275.0 + 0.9 * 0.0 - 274.9999999999999 )\n",
      "The new value of ((1, 250, 'R', 'L'), 'B') is  275.0\n",
      "END OF THE EPISODE _______________________________________________\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "State: depth: 0 , Bigger Column: 50 , First column to appear: L\n",
      "current_state setted to None\n",
      "Player looks for best value\n",
      "the agent chose the column that is  S\n",
      "The gain is incremented to 250\n",
      "The agent chose the smaller column, giving a reward of 70.0\n",
      "The current acumulated reward is 30.0\n",
      "Inmediate reward: 30.0\n",
      "update 277.0630621494673 =  277.0630621494673 + 0.8 *( 30.0 + 0.9 * 275.0 - 277.0630621494673 )\n",
      "The new value of ((0, 50, 'L', 'L'), 'S') is  277.41261242989344\n",
      "\n",
      "\n",
      "State: depth: 1 , Bigger Column: 250 , First column to appear: R\n",
      "current_state setted to ((0, 50, 'L', 'L'), 'S')\n",
      "exploratory\n",
      "the agent chose the column that is  S\n",
      "The gain is incremented to 1250\n",
      "The agent chose the smaller column, giving a reward of 275.0\n",
      "The current acumulated reward is 255.0\n",
      "Inmediate reward: 225.0\n",
      "update 0.0 =  0.0 + 0.8 *( 225.0 + 0.9 * 0.0 - 0.0 )\n",
      "The new value of ((1, 250, 'R', 'L'), 'S') is  180.0\n",
      "END OF THE EPISODE _______________________________________________\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "State: depth: 0 , Bigger Column: 50 , First column to appear: R\n",
      "current_state setted to None\n",
      "Player looks for best value\n",
      "the agent chose the column that is  S\n",
      "The gain is incremented to 250\n",
      "The agent chose the smaller column, giving a reward of 70.0\n",
      "The current acumulated reward is 30.0\n",
      "Inmediate reward: 30.0\n",
      "update 241.48569487143914 =  241.48569487143914 + 0.8 *( 30.0 + 0.9 * 275.0 - 241.48569487143914 )\n",
      "The new value of ((0, 50, 'R', 'L'), 'S') is  270.2971389742878\n",
      "\n",
      "\n",
      "State: depth: 1 , Bigger Column: 250 , First column to appear: R\n",
      "current_state setted to ((0, 50, 'R', 'L'), 'S')\n",
      "Player looks for best value\n",
      "the agent chose the column that is  B\n",
      "the gain is reduced to 50.0\n",
      "The agent chose the bigger column, , giving a reward of 275.0\n",
      "The current acumulated reward is 305.0\n",
      "Inmediate reward: 275.0\n",
      "update 275.0 =  275.0 + 0.8 *( 275.0 + 0.9 * 0.0 - 275.0 )\n",
      "The new value of ((1, 250, 'R', 'L'), 'B') is  275.0\n",
      "END OF THE EPISODE _______________________________________________\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "State: depth: 0 , Bigger Column: 50 , First column to appear: R\n",
      "current_state setted to None\n",
      "Player looks for best value\n",
      "the agent chose the column that is  S\n",
      "The gain is incremented to 250\n",
      "The agent chose the smaller column, giving a reward of 70.0\n",
      "The current acumulated reward is 30.0\n",
      "Inmediate reward: 30.0\n",
      "update 270.2971389742878 =  270.2971389742878 + 0.8 *( 30.0 + 0.9 * 274.99997184 - 270.2971389742878 )\n",
      "The new value of ((0, 50, 'R', 'L'), 'S') is  276.05940751965755\n",
      "\n",
      "\n",
      "State: depth: 1 , Bigger Column: 250 , First column to appear: R\n",
      "current_state setted to ((0, 50, 'R', 'L'), 'S')\n",
      "Player looks for best value\n",
      "the agent chose the column that is  B\n",
      "the gain is reduced to 50.0\n",
      "The agent chose the bigger column, , giving a reward of 275.0\n",
      "The current acumulated reward is 305.0\n",
      "Inmediate reward: 275.0\n",
      "update 274.99997184 =  274.99997184 + 0.8 *( 275.0 + 0.9 * 0.0 - 274.99997184 )\n",
      "The new value of ((1, 250, 'R', 'R'), 'B') is  274.999994368\n",
      "END OF THE EPISODE _______________________________________________\n",
      "\n",
      "\n",
      "20480.0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>horizon</th>\n",
       "      <th>current_depth</th>\n",
       "      <th>chosen_big</th>\n",
       "      <th>chosen_right</th>\n",
       "      <th>chosen_first</th>\n",
       "      <th>performance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>200 rows  6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    horizon current_depth chosen_big chosen_right chosen_first performance\n",
       "0         2             1      False         True        False           1\n",
       "1         2             2       True        False         True           1\n",
       "2         2             1      False         True         True           1\n",
       "3         2             2       True        False         True           1\n",
       "4         2             1       True        False        False           0\n",
       "..      ...           ...        ...          ...          ...         ...\n",
       "195       2             2      False         True         True           0\n",
       "196       2             1      False         True        False           1\n",
       "197       2             2       True        False        False           1\n",
       "198       2             1      False         True        False           1\n",
       "199       2             2       True        False         True           1\n",
       "\n",
       "[200 rows x 6 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "qlearning = QlearningAgent(learning_rate=0.8, exploration_factor=0.8, print_debug = print_debug, exploration_factor_progression=0.96)\n",
    "rewards = []\n",
    "df = None\n",
    "for i in range(100):\n",
    "    game = ColumnGame(qlearning, depth=2)\n",
    "    df_game = game.start_game()\n",
    "    df = pd.concat([df, df_game], ignore_index=True)\n",
    "    rewards.append(qlearning.accumulated_reward)\n",
    "# Prints the list of rewards and shows the final reward\n",
    "df.to_csv(\"qlearning-output.csv\")\n",
    "plt.plot(rewards)\n",
    "print(rewards[-1])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(float,\n",
       "            {((0, 50, 'R', 'L'), 'B'): 79.23427328000001,\n",
       "             ((1, 10.0, 'R', 'L'), 'B'): 10.99648,\n",
       "             ((1, 10.0, 'R', 'L'), 'S'): 7.2,\n",
       "             ((2, 2.0, 'L', 'L'), 'B'): 0.0,\n",
       "             ((2, 2.0, 'L', 'L'), 'S'): 0.0,\n",
       "             ((0, 50, 'L', 'L'), 'S'): 232.56045714603704,\n",
       "             ((1, 250, 'L', 'L'), 'B'): 220.0,\n",
       "             ((1, 250, 'L', 'L'), 'S'): 224.99999999999991,\n",
       "             ((2, 1250, 'R', 'R'), 'B'): 0.0,\n",
       "             ((2, 1250, 'R', 'R'), 'S'): 0.0,\n",
       "             ((0, 50, 'R', 'R'), 'B'): 56.0,\n",
       "             ((1, 10.0, 'R', 'R'), 'B'): 0.0,\n",
       "             ((1, 10.0, 'R', 'R'), 'S'): 8.9998848,\n",
       "             ((2, 50.0, 'R', 'L'), 'B'): 0.0,\n",
       "             ((2, 50.0, 'R', 'L'), 'S'): 0.0,\n",
       "             ((1, 250, 'R', 'R'), 'B'): 274.9999999999279,\n",
       "             ((1, 250, 'R', 'R'), 'S'): 0.0,\n",
       "             ((0, 50, 'R', 'R'), 'S'): 275.7023959625736,\n",
       "             ((2, 1250, 'R', 'L'), 'B'): 0.0,\n",
       "             ((2, 1250, 'R', 'L'), 'S'): 0.0,\n",
       "             ((0, 50, 'R', 'L'), 'S'): 268.84612865042396,\n",
       "             ((1, 250, 'R', 'L'), 'B'): 274.99999977472,\n",
       "             ((1, 250, 'R', 'L'), 'S'): 180.0,\n",
       "             ((1, 250, 'L', 'R'), 'B'): 0.0,\n",
       "             ((1, 250, 'L', 'R'), 'S'): 224.99999999994102,\n",
       "             ((0, 50, 'L', 'L'), 'B'): 0.0,\n",
       "             ((2, 50.0, 'L', 'L'), 'B'): 0.0,\n",
       "             ((2, 50.0, 'L', 'L'), 'S'): 0.0,\n",
       "             ((1, 10.0, 'L', 'L'), 'B'): 10.99997184,\n",
       "             ((1, 10.0, 'L', 'L'), 'S'): 0.0,\n",
       "             ((2, 2.0, 'R', 'R'), 'B'): 0.0,\n",
       "             ((2, 2.0, 'R', 'R'), 'S'): 0.0,\n",
       "             ((2, 50.0, 'R', 'R'), 'B'): 0.0,\n",
       "             ((2, 50.0, 'R', 'R'), 'S'): 0.0,\n",
       "             ((0, 50, 'L', 'R'), 'B'): 79.86832344569139,\n",
       "             ((0, 50, 'L', 'R'), 'S'): 239.98797214676458,\n",
       "             ((2, 2.0, 'L', 'R'), 'B'): 0.0,\n",
       "             ((2, 2.0, 'L', 'R'), 'S'): 0.0,\n",
       "             ((2, 50.0, 'L', 'R'), 'B'): 0.0,\n",
       "             ((2, 50.0, 'L', 'R'), 'S'): 0.0,\n",
       "             ((2, 1250, 'L', 'L'), 'B'): 0.0,\n",
       "             ((2, 1250, 'L', 'L'), 'S'): 0.0,\n",
       "             ((1, 10.0, 'L', 'R'), 'B'): 10.99648,\n",
       "             ((1, 10.0, 'L', 'R'), 'S'): 7.2,\n",
       "             ((2, 2.0, 'R', 'L'), 'B'): 0.0,\n",
       "             ((2, 2.0, 'R', 'L'), 'S'): 0.0,\n",
       "             ((2, 1250, 'L', 'R'), 'B'): 0.0,\n",
       "             ((2, 1250, 'L', 'R'), 'S'): 0.0})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qlearning.q_table\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TFG-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
